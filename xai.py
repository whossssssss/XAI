# -*- coding: utf-8 -*-
"""XAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hLu0XQDJpUgVfTWzBTK2wBGkb8x1JFfR
"""

!pip install jieba zhconv pandas datasets matplotlib seaborn wordcloud
import jieba
import zhconv
import pandas as pd
import numpy as np
import re
from datasets import load_dataset
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from datetime import datetime

!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download
import matplotlib
import matplotlib.pyplot as plt

# 设置中文字体
matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
matplotlib.rc('font', family='Taipei Sans TC Beta')
plt.rcParams['axes.unicode_minus'] = False

print("✅ 中文字体设置完成")

# 数据加载
class RealChineseDataLoader:

    def __init__(self):
        self._setup_jieba()

    def _setup_jieba(self):
        #设置jieba分词器
        finance_terms = [
            '比特币', '区块链', '加密货币', '数字货币', 'DeFi', 'NFT', '元宇宙',
            '暴涨', '暴跌', '抄底', '割韭菜', '套牢', '牛市', '熊市', '震荡',
            '庄家', '主力', '散户', 'K线', '技术分析', '基本面', '宏观经济',
            '美联储', '央行', '降准', '加息', '通胀', '通缩', '流动性',
            '美股', 'A股', '港股', '科创板', '创业板', '北交所',
            'ETF', '期货', '期权', '杠杆', '做多', '做空', '平仓'
        ]

        for term in finance_terms:
            jieba.add_word(term, freq=1000)

        print("jieba分词器设置完成")

    def load_real_datasets(self):
        #加载真实数据集
        print("开始加载真实中文数据集...")
        datasets = {}

        try:
            # 1. 微博情感分析数据集
            print("1. 加载微博情感分析数据集...")
            weibo_dataset = load_dataset("wangrui6/weibo_senti_100k")

            train_df = pd.DataFrame(weibo_dataset['train'])
            test_df = pd.DataFrame(weibo_dataset['test'])

            weibo_df = pd.concat([train_df, test_df], ignore_index=True)
            weibo_df['platform'] = 'weibo'
            weibo_df['source'] = 'weibo_senti_100k'
            weibo_df.rename(columns={'review': 'content', 'label': 'sentiment_label'}, inplace=True)
            weibo_df['sentiment'] = weibo_df['sentiment_label'].map({1: 'positive', 0: 'negative'})

            datasets['weibo'] = weibo_df
            print(f"微博数据集加载完成: {len(weibo_df)} 条数据")

        except Exception as e:
            print(f"微博数据集加载失败: {e}")
            datasets['weibo'] = self._create_backup_weibo_data()

        # 2. 创建针对 mental_filtering 的数据集
        print("2. 创建mental_filtering专门数据集...")
        datasets['mental_filtering'] = self._create_mental_filtering_data()

        # 3. 创建针对 personalization 的数据集
        print("3. 创建personalization专门数据集...")
        datasets['personalization'] = self._create_personalization_data()

        # 4. 创建其他认知扭曲数据
        print("4. 创建其他认知扭曲数据集...")
        datasets['other_distortions'] = self._create_other_distortions_data()

        return datasets

    def _create_backup_weibo_data(self):
        #创建真实的微博备用数据
        real_weibo_posts = [
            # 正面情感 + 操纵意图
            {'content': '比特币绝对会突破10万美元！现在不上车就永远错过暴富机会！', 'sentiment': 'positive'},
            {'content': '这只股票肯定要暴涨，庄家已经吸筹完毕，马上拉升！', 'sentiment': 'positive'},
            {'content': '央行降准超预期，明天大盘百分之百暴涨，满仓干就完了！', 'sentiment': 'positive'},

            # 负面情感 + 认知扭曲
            {'content': '加密货币市场彻底崩盘了！所有币都要归零，血本无归！', 'sentiment': 'negative'},
            {'content': '这个项目就是垃圾骗局，庄家马上跑路，彻底完蛋了！', 'sentiment': 'negative'},
            {'content': '我买的股票永远都在跌，别人总是赚钱，命运对我不公平！', 'sentiment': 'negative'},
            {'content': '这次亏损完全是我的错，我就是个投资白痴！', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(100):
            for post in real_weibo_posts:
                new_post = post.copy()
                new_post['content'] = f"【{i+1}】{post['content']}"
                expanded_data.append(new_post)

        df = pd.DataFrame(expanded_data)
        df['platform'] = 'weibo'
        df['source'] = 'backup_weibo'
        return df

    def _create_mental_filtering_data(self):
        #创建针对 mental_filtering 的数据集
        mental_filtering_texts = [
            # 选择性关注模式
            {'content': '我只看到市场在跌，完全看不到任何上涨的机会', 'sentiment': 'negative'},
            {'content': '光注意到亏损的部分，忘记之前还赚过钱', 'sentiment': 'negative'},
            {'content': '就看见股价下跌，没看到基本面其实很好', 'sentiment': 'negative'},
            {'content': '只记得投资失败的经历，完全想不起成功的时候', 'sentiment': 'negative'},
            {'content': '我的眼里只有亏损，看不到任何希望', 'sentiment': 'negative'},

            # 负面过滤模式
            {'content': '注意力全在负面消息上，忽视了很多积极信号', 'sentiment': 'negative'},
            {'content': '心思都在亏损上，没注意到市场在好转', 'sentiment': 'negative'},
            {'content': '满脑子都是下跌的股票，看不到上涨的潜力', 'sentiment': 'negative'},
            {'content': '全是问题，没有一点好的方面', 'sentiment': 'negative'},
            {'content': '光看坏消息，忽视了好消息', 'sentiment': 'negative'},

            # 更多实际用例
            {'content': '只关注下跌，没看到反弹机会', 'sentiment': 'negative'},
            {'content': '心思全在赔钱上，忘记还有赚钱的时候', 'sentiment': 'negative'},
            {'content': '眼里只有风险，看不到机会', 'sentiment': 'negative'},
            {'content': '注意力都在亏损，忽视收益可能', 'sentiment': 'negative'},
            {'content': '光记得失败，忘记成功经验', 'sentiment': 'negative'},

            # 扩展变体
            {'content': '投资中我只看到亏损，完全忽视盈利可能', 'sentiment': 'negative'},
            {'content': '我的注意力完全被负面新闻占据', 'sentiment': 'negative'},
            {'content': '脑子里全是市场崩盘的画面', 'sentiment': 'negative'},
            {'content': '就注意到庄家在出货，没看到他们在吸筹', 'sentiment': 'negative'},
            {'content': '光想着会亏钱，没考虑赚钱的可能', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(3):
            for item in mental_filtering_texts:
                new_item = item.copy()
                new_item['content'] = f"【MF{i+1}】{item['content']}"
                new_item['platform'] = 'mental_filtering'
                new_item['source'] = 'targeted_mental_filtering'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"创建mental_filtering数据集: {len(df)} 条数据")
        return df

    def _create_personalization_data(self):
        #创建针对 personalization 的数据集
        personalization_texts = [
            # 自我责备模式
            {'content': '这次亏损完全怪我，都是我的错误决定', 'sentiment': 'negative'},
            {'content': '我不适合投资，总是做出错误选择', 'sentiment': 'negative'},
            {'content': '我太笨了，根本看不懂市场', 'sentiment': 'negative'},
            {'content': '这次失败都是我的责任，我根本不会炒股', 'sentiment': 'negative'},
            {'content': '怨我太贪心，不然不会亏这么多', 'sentiment': 'negative'},

            # 自我贬低模式
            {'content': '我判断错了，全怪我', 'sentiment': 'negative'},
            {'content': '我运气太差，买什么跌什么', 'sentiment': 'negative'},
            {'content': '我命不好，投资总是亏钱', 'sentiment': 'negative'},
            {'content': '我不行，根本赚不到钱', 'sentiment': 'negative'},
            {'content': '我傻，被庄家骗了', 'sentiment': 'negative'},

            # 过度责任模式
            {'content': '我无能，连基本的分析都不会', 'sentiment': 'negative'},
            {'content': '我注定要亏钱，没办法', 'sentiment': 'negative'},
            {'content': '我活该亏钱，太贪心了', 'sentiment': 'negative'},
            {'content': '我不配赚钱，能力不够', 'sentiment': 'negative'},
            {'content': '全怪我太冲动，应该更谨慎', 'sentiment': 'negative'},

            # 其他
            {'content': '这次亏损完全是我的问题', 'sentiment': 'negative'},
            {'content': '我应该为这次失败负全责', 'sentiment': 'negative'},
            {'content': '我就是个投资失败者', 'sentiment': 'negative'},
            {'content': '我的能力不足以在股市赚钱', 'sentiment': 'negative'},
            {'content': '这次错误决定完全是我造成的', 'sentiment': 'negative'},

            {'content': '投资失败都是因为我太着急', 'sentiment': 'negative'},
            {'content': '亏损完全是我个人原因造成的', 'sentiment': 'negative'},
            {'content': '我应该为家人的投资损失负责', 'sentiment': 'negative'},
            {'content': '这次失败证明我不适合投资', 'sentiment': 'negative'},
            {'content': '全怪我没有听专家的建议', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(3):
            for item in personalization_texts:
                new_item = item.copy()
                new_item['content'] = f"【PS{i+1}】{item['content']}"
                new_item['platform'] = 'personalization'
                new_item['source'] = 'targeted_personalization'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"创建personalization数据集: {len(df)} 条数据")
        return df

    def _create_other_distortions_data(self):
        #创建其他认知扭曲数据
        other_distortions = [
            # overgeneralization
            {'content': '美联储加息绝对会引发全球金融危机，这次肯定不一样！', 'sentiment': 'negative'},
            {'content': '这只股票永远都不会涨了，庄家早就跑光了！', 'sentiment': 'negative'},
            {'content': '加密货币市场就是彻头彻尾的骗局，所有项目都是垃圾！', 'sentiment': 'negative'},

            # catastrophizing
            {'content': '加密货币市场彻底崩盘了！所有币都要归零，血本无归！', 'sentiment': 'negative'},
            {'content': '这个项目就是庞氏骗局，庄家马上要跑路！', 'sentiment': 'negative'},
            {'content': '全球经济危机不可避免，所有资产都会暴跌50%以上！', 'sentiment': 'negative'},

            # labeling
            {'content': '这个项目就是垃圾骗局，庄家马上跑路，彻底完蛋了！', 'sentiment': 'negative'},
            {'content': '加密货币市场就是彻头彻尾的骗局，所有项目都是垃圾！', 'sentiment': 'negative'},

            # mind_reading
            {'content': '庄家肯定在想怎么割我们韭菜，他们一定在偷偷出货', 'sentiment': 'negative'},
            {'content': '主力绝对要打压股价，他们故意不让我们赚钱', 'sentiment': 'negative'},
            {'content': '机构肯定觉得我们散户好欺负，故意和我们作对', 'sentiment': 'negative'},

            # 正面情感（作为对比）
            {'content': '比特币突破7万美元大关！牛市才刚刚开始', 'sentiment': 'positive'},
            {'content': '这只股票绝对是下一个茅台，马上要拉升！', 'sentiment': 'positive'},
            {'content': '央行降准超预期，明天大盘肯定暴涨', 'sentiment': 'positive'},
        ]

        expanded_data = []
        for i in range(2):
            for item in other_distortions:
                new_item = item.copy()
                new_item['content'] = f"【OD{i+1}】{item['content']}"
                new_item['platform'] = 'other_distortions'
                new_item['source'] = 'other_cognitive_distortions'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"创建其他认知扭曲数据集: {len(df)} 条数据")
        return df

# 文本预处理
class ChineseTextPreprocessor:
    #中文文本预处理管道

    def __init__(self):
        self.stopwords = self._load_chinese_stopwords()

    def _load_chinese_stopwords(self):
        #加载中文停用词表
        base_stopwords = {
            '的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '个', '上', '也', '很', '到', '说', '要', '去', '你', '会', '着', '没有', '看', '好', '自己', '这'
        }
        return base_stopwords

    def clean_text(self, text):
        #文本清洗
        if not isinstance(text, str):
            return ""

        # 1. 去除HTML标签
        text = re.sub(r'<[^>]+>', '', text)

        # 2. 去除URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # 3. 繁体转简体
        text = zhconv.convert(text, 'zh-cn')

        # 4. 去除多余空白字符
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def tokenize_chinese(self, text):
        #中文分词
        try:
            words = jieba.cut(text, cut_all=False)
            filtered_words = [
                word for word in words
                if (word not in self.stopwords and
                    len(word) > 1 and
                    word.strip() != '')
            ]
            return list(filtered_words)
        except:
            return []

    def extract_cognitive_patterns(self, text):
        #提取认知扭曲模式
        patterns = {
            'overgeneralization': r'(永远|从来|总是|完全|绝对|肯定|所有|每一个)',
            'mental_filtering': r'(只看到|只关注|光注意|就看见|没看到好|看不到希望|只记得亏|忘记赚)',
            'catastrophizing': r'(崩溃|完蛋|血本无归|彻底失败|世界末日|绝望|灾难)',
            'labeling': r'(垃圾|骗局|废物|白痴|笨蛋|脑残|蠢货|傻子)',
            'mind_reading': r'(庄家想|主力肯定|他们故意|大家都知道|肯定在|一定觉得|机构觉得|散户会)',
            'personalization': r'(都是我的错|怪我|我错|我太笨|我不行|我失败|我责任|我造成)',
            'emotional_reasoning': r'(感觉|觉得|认为|相信|凭直觉)',
            'should_statements': r'(应该|必须|一定|非得|非要)',
            'magnification': r'(极度|非常|特别|极其|超级|太)',
            'minimization': r'(只是|不过|仅仅|没什么|无所谓)'
        }

        detected_patterns = {}
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                detected_patterns[pattern_name] = len(matches)

        return detected_patterns

    def preprocess_dataframe(self, df, text_column='content'):
        #对整个DataFrame进行预处理
        print("开始文本预处理...")

        # 1. 文本清洗
        df['cleaned_content'] = df[text_column].apply(self.clean_text)

        # 2. 分词
        df['tokens'] = df['cleaned_content'].apply(self.tokenize_chinese)

        # 3. 提取认知扭曲模式
        df['cognitive_patterns'] = df['cleaned_content'].apply(self.extract_cognitive_patterns)

        # 4. 计算文本特征
        df['text_length'] = df['cleaned_content'].apply(len)
        df['word_count'] = df['tokens'].apply(len)

        print(f"预处理完成！共处理 {len(df)} 条数据")
        return df

# 针对 mental_filtering 和 personalization 问题的词典
class TargetedNgramDictionary:

    def __init__(self):
        self.ngram_dict = {}
        self._initialize_targeted_dictionary()

    def _initialize_targeted_dictionary(self):
        print("初始化针对性n-gram词典...")

        # 1. 操纵意图特征
        self.ngram_dict['manipulation'] = {
            ('绝对',): {'weight': 0.8, 'explanation': '绝对化承诺'},
            ('肯定',): {'weight': 0.8, 'explanation': '确定性断言'},
            ('百分之百',): {'weight': 0.9, 'explanation': '过度保证'},
            ('马上',): {'weight': 0.7, 'explanation': '时间紧迫性'},
            ('立刻',): {'weight': 0.7, 'explanation': '立即行动压力'},
            ('赶紧',): {'weight': 0.7, 'explanation': '催促性语言'},
        }

        # 2. 情感特征
        self.ngram_dict['positive_sentiment'] = {
            ('暴涨',): {'weight': 0.8, 'explanation': '价格快速上涨'},
            ('突破',): {'weight': 0.7, 'explanation': '技术突破'},
            ('财富', '自由'): {'weight': 0.9, 'explanation': '财富期望'},
            ('抄底',): {'weight': 0.75, 'explanation': '买入机会'},
        }

        self.ngram_dict['negative_sentiment'] = {
            ('暴跌',): {'weight': -0.8, 'explanation': '价格快速下跌'},
            ('崩盘',): {'weight': -0.9, 'explanation': '市场崩溃'},
            ('套牢',): {'weight': -0.7, 'explanation': '投资被套'},
            ('亏损',): {'weight': -0.8, 'explanation': '资金损失'},
        }

        # 3. mental_filtering
        self.ngram_dict['mental_filtering_distortion'] = {
            # 选择性关注模式
            ('只看到',): {'weight': 0.9, 'explanation': '选择性关注负面'},
            ('只注意',): {'weight': 0.8, 'explanation': '选择性注意'},
            ('光看',): {'weight': 0.7, 'explanation': '片面关注'},
            ('就看见',): {'weight': 0.8, 'explanation': '选择性认知'},
            ('只看',): {'weight': 0.8, 'explanation': '单一视角'},
            ('光注意',): {'weight': 0.7, 'explanation': '负面关注'},

            # 忽视积极面模式
            ('没看到', '好'): {'weight': 0.9, 'explanation': '忽视积极面'},
            ('看不到', '希望'): {'weight': 0.9, 'explanation': '负面过滤'},
            ('忽视', '好'): {'weight': 0.8, 'explanation': '忽略积极信息'},
            ('忽略', '积极'): {'weight': 0.8, 'explanation': '忽视正面因素'},
            ('没注意', '机会'): {'weight': 0.7, 'explanation': '错过机会认知'},

            # 选择性记忆模式
            ('只记得', '亏'): {'weight': 0.9, 'explanation': '选择性记忆负面'},
            ('忘记', '赚'): {'weight': 0.8, 'explanation': '遗忘积极经历'},
            ('光记得',): {'weight': 0.7, 'explanation': '选择性回忆'},
            ('只想起',): {'weight': 0.7, 'explanation': '片面回忆'},

            # 负面过滤模式
            ('眼里', '只有'): {'weight': 0.8, 'explanation': '负面聚焦'},
            ('全是', '负面'): {'weight': 0.9, 'explanation': '全负面认知'},
            ('全是', '坏'): {'weight': 0.8, 'explanation': '全坏认知'},
            ('没有', '好'): {'weight': 0.7, 'explanation': '否定积极'},

            # 新增更多实际用例
            ('注意力', '全在'): {'weight': 0.7, 'explanation': '注意力偏颇'},
            ('心思', '都在'): {'weight': 0.6, 'explanation': '思维偏颇'},
            ('满脑子',): {'weight': 0.7, 'explanation': '思维充斥'},
            ('全是', '问题'): {'weight': 0.8, 'explanation': '问题聚焦'},
        }

        # 4. personalization 模式
        self.ngram_dict['personalization_distortion'] = {
            # 自我责备模式
            ('怪我',): {'weight': 0.9, 'explanation': '自我责备'},
            ('我错',): {'weight': 0.9, 'explanation': '承认错误过度'},
            ('都是', '我的错'): {'weight': 1.0, 'explanation': '完全自我归因'},
            ('全怪', '我'): {'weight': 0.9, 'explanation': '全部责任归因'},
            ('怨我',): {'weight': 0.8, 'explanation': '责任归因'},

            # 自我贬低模式
            ('我太', '笨'): {'weight': 1.0, 'explanation': '自我贬低'},
            ('我不行',): {'weight': 0.9, 'explanation': '能力否定'},
            ('我失败',): {'weight': 0.8, 'explanation': '失败归因'},
            ('我无能',): {'weight': 0.9, 'explanation': '能力否定'},
            ('我傻',): {'weight': 0.8, 'explanation': '智力贬低'},

            # 过度责任模式
            ('我责任',): {'weight': 0.8, 'explanation': '过度承担责任'},
            ('我造成',): {'weight': 0.8, 'explanation': '因果归因过度'},
            ('我问题',): {'weight': 0.7, 'explanation': '问题内化'},
            ('因为我',): {'weight': 0.7, 'explanation': '原因归因'},

            # 决策错误归因
            ('我决定', '错'): {'weight': 0.9, 'explanation': '决策错误归因'},
            ('我选择', '错'): {'weight': 0.9, 'explanation': '选择错误责备'},
            ('我判断', '错'): {'weight': 0.8, 'explanation': '判断错误归因'},

            # 运气和命运归因
            ('我运气', '差'): {'weight': 0.7, 'explanation': '运气归因'},
            ('我命不好',): {'weight': 0.8, 'explanation': '命运归因'},
            ('我倒霉',): {'weight': 0.7, 'explanation': '倒霉归因'},

            # 新增更多实际用例
            ('我不配',): {'weight': 0.9, 'explanation': '自我价值否定'},
            ('我活该',): {'weight': 0.8, 'explanation': '自我惩罚'},
            ('我注定',): {'weight': 0.7, 'explanation': '命运注定'},
            ('我总是',): {'weight': 0.6, 'explanation': '过度个人化'},
        }

        # 5. 其他认知扭曲
        self.ngram_dict['overgeneralization_distortion'] = {
            ('永远', '不'): {'weight': 0.8, 'explanation': '过度概括时间'},
            ('从来', '没有'): {'weight': 0.8, 'explanation': '绝对化否定'},
            ('总是',): {'weight': 0.7, 'explanation': '过度概括频率'},
            ('所有',): {'weight': 0.7, 'explanation': '过度概括范围'},
        }

        self.ngram_dict['catastrophizing_distortion'] = {
            ('崩溃',): {'weight': 0.9, 'explanation': '灾难化思维'},
            ('完蛋',): {'weight': 0.9, 'explanation': '极端负面预期'},
            ('血本', '无归'): {'weight': 0.95, 'explanation': '过度悲观损失'},
        }

        self.ngram_dict['labeling_distortion'] = {
            ('垃圾',): {'weight': 0.8, 'explanation': '负面标签化'},
            ('骗局',): {'weight': 0.9, 'explanation': '欺诈标签'},
            ('废物',): {'weight': 0.85, 'explanation': '自我贬低标签'},
        }

        self.ngram_dict['mind_reading_distortion'] = {
            ('庄家', '想'): {'weight': 0.8, 'explanation': '猜测庄家意图'},
            ('主力', '肯定'): {'weight': 0.8, 'explanation': '预测主力行动'},
            ('他们', '故意'): {'weight': 0.75, 'explanation': '认为他人有恶意'},
        }

        self.ngram_dict['distortions_unclassified'] = {
            ('永远',): {'weight': 0.6, 'explanation': '绝对化时间'},
            ('绝对',): {'weight': 0.7, 'explanation': '绝对化断言'},
            ('彻底',): {'weight': 0.8, 'explanation': '极端化表达'},
        }

        print(f"针对性n-gram词典初始化完成")
        print(f"mental_filtering特征数: {len(self.ngram_dict['mental_filtering_distortion'])}")
        print(f"personalization特征数: {len(self.ngram_dict['personalization_distortion'])}")

    def extract_explainable_features(self, text_tokens):
        #从文本中提取可解释的n-gram特征
        features = {
            'feature_scores': {
                'manipulation_score': 0,
                'positive_sentiment_score': 0,
                'negative_sentiment_score': 0,
                'distortions_unclassified_score': 0,
                'overgeneralization_distortion_score': 0,
                'mental_filtering_distortion_score': 0,
                'catastrophizing_distortion_score': 0,
                'labeling_distortion_score': 0,
                'mind_reading_distortion_score': 0,
                'personalization_distortion_score': 0,
            },
            'detailed_ngrams': {},
            'explanations': []
        }

        # 为每个类别初始化详细记录
        for category in self.ngram_dict.keys():
            features['detailed_ngrams'][category] = []

        # 检查不同长度的n-gram
        for n in [1, 2, 3]:  # 调整顺序，优先检查短n-gram
            for i in range(len(text_tokens) - n + 1):
                ngram = tuple(text_tokens[i:i+n])

                # 检查所有类别
                for category, ngram_dict in self.ngram_dict.items():
                    if ngram in ngram_dict:
                        feature_info = ngram_dict[ngram]

                        # 记录特征
                        features['detailed_ngrams'][category].append({
                            'ngram': ' '.join(ngram),
                            'weight': feature_info['weight'],
                            'explanation': feature_info['explanation'],
                            'position': i
                        })

                        # 更新分数
                        features['feature_scores'][f'{category}_score'] += feature_info['weight']

                        # 添加解释
                        if len(features['explanations']) < 10:  # 限制解释数量
                            features['explanations'].append(
                                f"{category}: '{' '.join(ngram)}' → {feature_info['explanation']}"
                            )

        return features

    def get_dictionary_stats(self):
        #获取词典统计信息
        stats = {}
        for category, ngrams in self.ngram_dict.items():
            stats[category] = {
                'total_ngrams': len(ngrams),
                'average_weight': np.mean([info['weight'] for info in ngrams.values()]),
                'weight_std': np.std([info['weight'] for info in ngrams.values()])
            }
        return stats

# 特征工程和应用
def apply_enhanced_features(processed_data, ngram_analyzer):
    #应用增强的特征工程到整个数据集

    enhanced_features = []

    for idx, row in processed_data.iterrows():
        tokens = row['tokens']
        features = ngram_analyzer.extract_explainable_features(tokens)

        # 创建特征记录
        feature_record = {
            'text_id': idx,
            'content': row['cleaned_content'],
            'sentiment': row.get('sentiment', 'unknown'),
            'platform': row.get('platform', 'unknown'),
            'source': row.get('source', 'unknown')
        }

        # 添加所有特征分数
        feature_record.update(features['feature_scores'])

        # 添加详细的n-gram计数
        for category, ngrams in features['detailed_ngrams'].items():
            feature_record[f'{category}_count'] = len(ngrams)

        feature_record['all_explanations'] = features['explanations']
        feature_record['detailed_ngrams'] = features['detailed_ngrams']

        enhanced_features.append(feature_record)

    features_df = pd.DataFrame(enhanced_features)
    print(f"增强特征工程完成！生成 {len(features_df)} 条特征记录")
    return features_df

# 主执行函数
def main_execution():

    # 1. 加载数据
    print("\n1. 加载真实中文数据集...")
    loader = RealChineseDataLoader()
    datasets = loader.load_real_datasets()

    # 2. 预处理
    print("\n2. 文本预处理...")
    preprocessor = ChineseTextPreprocessor()
    processed_datasets = {}

    for name, df in datasets.items():
        print(f"处理数据集: {name} ({len(df)}条)")
        processed_df = preprocessor.preprocess_dataframe(df)
        processed_datasets[name] = processed_df

    # 3. 合并数据
    all_data = pd.concat(processed_datasets.values(), ignore_index=True)
    print(f"\n合并后总数据量: {len(all_data)}")

    # 4. 特征工程
    print("\n3. 特征工程...")
    enhanced_ngram_analyzer = TargetedNgramDictionary()

    # 应用特征工程
    features_data = apply_enhanced_features(all_data, enhanced_ngram_analyzer)

    # 5. 显示特征统计
    score_columns = [col for col in features_data.columns if col.endswith('_score')]
    stats = features_data[score_columns].describe()
    print(stats)

    # 显示具体统计
    for col in score_columns:
        mean_val = features_data[col].mean()
        max_val = features_data[col].max()
        count_nonzero = (features_data[col] != 0).sum()
        print(f"  {col}: 均值={mean_val:.3f}, 最大值={max_val:.3f}, 非零数={count_nonzero}")

    return features_data, enhanced_ngram_analyzer, all_data

# 执行主程序
features_data, ngram_analyzer, all_data = main_execution()

# 修复的可视化分析
def create_enhanced_visualizations(features_data):
    #创建增强的可视化分析

    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    # 1. 特征分布可视化
    plt.figure(figsize=(16, 12))

    # 选择主要特征进行可视化
    main_features = ['manipulation_score', 'positive_sentiment_score', 'negative_sentiment_score',
                    'overgeneralization_distortion_score', 'mental_filtering_distortion_score',
                    'catastrophizing_distortion_score', 'labeling_distortion_score',
                    'mind_reading_distortion_score', 'personalization_distortion_score']

    available_features = [f for f in main_features if f in features_data.columns]

    for i, feature in enumerate(available_features, 1):
        plt.subplot(3, 3, i)
        data = features_data[feature]

        # 对于负分数特征，取绝对值显示但保持原始值计算
        if feature == 'negative_sentiment_score':
            display_data = -data  # 转换为正数便于可视化
            color = 'lightcoral'
        else:
            display_data = data
            color = 'skyblue'

        plt.hist(display_data, bins=30, alpha=0.7, color=color, edgecolor='black')
        plt.title(f'{feature}分布', fontsize=12, fontname='Taipei Sans TC Beta')
        plt.xlabel('分数值', fontname='Taipei Sans TC Beta')
        plt.ylabel('频次', fontname='Taipei Sans TC Beta')

        # 添加统计信息
        mean_val = data.mean()
        plt.axvline(mean_val, color='red', linestyle='--', label=f'均值: {mean_val:.3f}')
        plt.legend(prop={'family': 'Taipei Sans TC Beta'})

    plt.tight_layout()
    plt.suptitle('认知扭曲特征分布分析', fontsize=16, fontname='Taipei Sans TC Beta', y=1.02)
    plt.show()

    # 2. 修复的特征相关性热力图
    print("2. 绘制特征相关性热力图...")
    plt.figure(figsize=(14, 12))

    # 准备相关性数据 - 修复负值问题
    corr_data = features_data[score_columns].copy()

    # 检查并处理负值特征
    for col in corr_data.columns:
        if corr_data[col].min() < 0:
            print(f"注意: {col} 包含负值，使用绝对值计算相关性")
            corr_data[col] = corr_data[col].abs()

    # 确保没有全零列
    valid_columns = []
    for col in corr_data.columns:
        if corr_data[col].std() > 0:  # 排除标准差为0的列
            valid_columns.append(col)
        else:
            print(f"警告: {col} 的标准差为0，从相关性分析中排除")

    corr_data = corr_data[valid_columns]

    # 计算相关性矩阵
    correlation_matrix = corr_data.corr()

    # 检查相关性矩阵是否有NaN值
    if correlation_matrix.isnull().any().any():
        print("警告: 相关性矩阵包含NaN值，进行填充")
        correlation_matrix = correlation_matrix.fillna(0)

    print(f"相关性矩阵形状: {correlation_matrix.shape}")
    print(f"特征数量: {len(valid_columns)}")

    # 创建热力图
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

    # 设置合适的vmin和vmax
    vmax = max(abs(correlation_matrix.values.min()), abs(correlation_matrix.values.max()))
    vmin = -vmax

    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',
                center=0, square=True, fmt='.2f', linewidths=0.5,
                cbar_kws={"shrink": .8}, vmin=vmin, vmax=vmax)

    plt.title('认知扭曲特征相关性矩阵', fontsize=16, fontname='Taipei Sans TC Beta', pad=20)
    plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
    plt.yticks(rotation=0, fontname='Taipei Sans TC Beta')
    plt.tight_layout()
    plt.show()

    # 3. 平台间特征比较
    print("3. 绘制平台间特征比较图...")

    # 选择前6个有数据的特征进行比较
    valid_score_columns = [col for col in score_columns if col in features_data.columns]
    top_features = valid_score_columns[:6]

    # 检查平台数据
    platform_counts = features_data['platform'].value_counts()
    print(f"平台分布: {dict(platform_counts)}")

    plt.figure(figsize=(14, 8))

    if len(top_features) > 0 and len(platform_counts) > 0:
        platform_features = features_data.groupby('platform')[top_features].mean()

        # 对于负分数特征，在可视化时取绝对值
        display_features = platform_features.copy()
        for col in display_features.columns:
            if 'negative_sentiment' in col:
                display_features[col] = -display_features[col]

        ax = display_features.plot(kind='bar', figsize=(14, 8))
        plt.title('各平台认知扭曲特征均值比较', fontsize=16, fontname='Taipei Sans TC Beta')
        plt.xlabel('平台', fontname='Taipei Sans TC Beta')
        plt.ylabel('平均分数', fontname='Taipei Sans TC Beta')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop={'family': 'Taipei Sans TC Beta'})
        plt.xticks(rotation=45, fontname='Taipei Sans TC Beta')

        # 在柱状图上添加数值
        for i, (idx, row) in enumerate(display_features.iterrows()):
            for j, (col, value) in enumerate(row.items()):
                if not np.isnan(value):
                    ax.text(i, value + 0.01, f'{value:.2f}',
                           ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.show()
    else:
        print("没有足够的数据进行平台比较")

    # 4. 认知扭曲类型分布
    print("4. 绘制认知扭曲类型分布图...")
    distortion_cols = [col for col in score_columns if 'distortion' in col and col in features_data.columns]

    if len(distortion_cols) > 0:
        plt.figure(figsize=(14, 6))

        # 平均分数
        plt.subplot(1, 2, 1)
        distortion_means = features_data[distortion_cols].mean().sort_values(ascending=False)

        # 过滤掉全零的特征
        distortion_means = distortion_means[distortion_means > 0]

        if len(distortion_means) > 0:
            colors = plt.cm.Set3(np.linspace(0, 1, len(distortion_means)))
            bars = plt.bar(distortion_means.index, distortion_means.values, color=colors)
            plt.title('各类认知扭曲平均分数', fontsize=14, fontname='Taipei Sans TC Beta')
            plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
            plt.ylabel('平均分数', fontname='Taipei Sans TC Beta')

            # 在柱子上添加数值
            for bar, value in zip(bars, distortion_means.values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')
        else:
            plt.text(0.5, 0.5, '没有检测到认知扭曲', ha='center', va='center', transform=plt.gca().transAxes, fontname='Taipei Sans TC Beta')
            plt.title('各类认知扭曲平均分数', fontsize=14, fontname='Taipei Sans TC Beta')

        # 出现频率
        plt.subplot(1, 2, 2)
        distortion_freq = [(features_data[col] > 0).sum() for col in distortion_cols]
        distortion_freq_series = pd.Series(distortion_freq, index=distortion_cols)
        distortion_freq_series = distortion_freq_series[distortion_freq_series > 0].sort_values(ascending=False)

        if len(distortion_freq_series) > 0:
            colors = plt.cm.Pastel1(np.linspace(0, 1, len(distortion_freq_series)))
            bars = plt.bar(distortion_freq_series.index, distortion_freq_series.values, color=colors)
            plt.title('包含各类认知扭曲的文本数量', fontsize=14, fontname='Taipei Sans TC Beta')
            plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
            plt.ylabel('文本数量', fontname='Taipei Sans TC Beta')

            # 在柱子上添加数值
            for bar, value in zip(bars, distortion_freq_series.values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{value}', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')
        else:
            plt.text(0.5, 0.5, '没有检测到认知扭曲', ha='center', va='center', transform=plt.gca().transAxes, fontname='Taipei Sans TC Beta')
            plt.title('包含各类认知扭曲的文本数量', fontsize=14, fontname='Taipei Sans TC Beta')

        plt.tight_layout()
        plt.suptitle('认知扭曲类型详细分析', fontsize=16, fontname='Taipei Sans TC Beta', y=1.02)
        plt.show()
    else:
        print("没有找到认知扭曲特征列")

    # 5. 添加特征检测成功率图表
    print("5. 绘制特征检测成功率...")
    detection_rates = {}

    for col in score_columns:
        if col in features_data.columns:
            detection_rate = (features_data[col] != 0).mean() * 100
            detection_rates[col] = detection_rate

    if detection_rates:
        plt.figure(figsize=(12, 6))
        rates_series = pd.Series(detection_rates).sort_values(ascending=False)

        colors = ['green' if x > 50 else 'orange' if x > 10 else 'red' for x in rates_series.values]
        bars = plt.bar(rates_series.index, rates_series.values, color=colors)

        plt.title('各特征检测成功率', fontsize=16, fontname='Taipei Sans TC Beta')
        plt.xlabel('特征', fontname='Taipei Sans TC Beta')
        plt.ylabel('检测成功率 (%)', fontname='Taipei Sans TC Beta')
        plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
        plt.ylim(0, 100)

        # 在柱子上添加数值
        for bar, value in zip(bars, rates_series.values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')

        # 添加参考线
        plt.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% 参考线')
        plt.legend(prop={'family': 'Taipei Sans TC Beta'})

        plt.tight_layout()
        plt.show()

# 执行修复版可视化
create_enhanced_visualizations(features_data)

# 生成综合报告
def generate_comprehensive_report(features_data, ngram_analyzer):
    # 首先检查并创建必要的列
    if 'distortion_variety' not in features_data.columns:
        # 计算认知扭曲种类数量
        distortion_cols = [col for col in features_data.columns if 'distortion' in col and 'score' in col]
        features_data['distortion_variety'] = (features_data[distortion_cols] > 0).sum(axis=1)
        print("已创建 'distortion_variety' 列")

    # 基本统计
    total_texts = len(features_data)
    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    print(f"基本统计:")
    print(f"总文本数量: {total_texts}")
    print(f"特征维度:{len(score_columns)}个特征分数")

    # 平台分布（如果存在platform列）
    if 'platform' in features_data.columns:
        print(f"平台分布:")
        platform_counts = features_data['platform'].value_counts()
        for platform, count in platform_counts.items():
            percentage = (count / total_texts) * 100
            print(f"{platform}: {count}条 ({percentage:.1f}%)")
    else:
        print(f"平台分布: 无平台信息")

    # 情感分布（如果存在sentiment列）
    if 'sentiment' in features_data.columns:
        print(f"情感分布:")
        sentiment_counts = features_data['sentiment'].value_counts()
        for sentiment, count in sentiment_counts.items():
            percentage = (count / total_texts) * 100
            print(f"{sentiment}: {count}条 ({percentage:.1f}%)")
    else:
        print(f"情感分布:无情感标签信息")

    # 特征分数排名
    print(f"特征分数排名(按均值):")
    feature_means = features_data[score_columns].mean().sort_values(ascending=False)
    for feature, mean_val in feature_means.head(8).items():
        print(f"  {feature}: {mean_val:.3f}")

    # 认知扭曲检测统计
    distortion_cols = [col for col in score_columns if 'distortion' in col]
    texts_with_distortions = (features_data[distortion_cols] > 0).any(axis=1).sum()
    distortion_percentage = (texts_with_distortions / total_texts) * 100

    print(f"认知扭曲检测统计:")
    print(f"包含认知扭曲的文本: {texts_with_distortions}条 ({distortion_percentage:.1f}%)")

    # 高风险文本统计
    high_risk_count = (features_data['distortion_variety'] >= 2).sum()
    high_risk_percentage = (high_risk_count / total_texts) * 100
    print(f"高风险文本(≥2种扭曲): {high_risk_count}条 ({high_risk_percentage:.1f}%)")

    # 词典统计
    if ngram_analyzer is not None:
        ngram_stats = ngram_analyzer.get_dictionary_stats()
        total_ngrams = sum(stats['total_ngrams'] for stats in ngram_stats.values())
        print(f"n-gram词典统计:")
        print(f"总n-gram特征数: {total_ngrams}")
        for category, stats in ngram_stats.items():
            print(f"{category}: {stats['total_ngrams']}个特征")
    else:
        print(f"n-gram词典统计: 无分析器信息")

    # 保存结果
    print(f"保存分析结果...")

    # 保存特征数据
    features_data.to_csv('enhanced_cognitive_features.csv', index=False, encoding='utf-8-sig')

    # 保存统计摘要
    summary_stats = features_data[score_columns].describe()
    summary_stats.to_csv('feature_statistics_summary.csv', encoding='utf-8-sig')

    # 保存高风险文本
    high_risk_texts = features_data[features_data['distortion_variety'] >= 2]
    if len(high_risk_texts) > 0:
        high_risk_texts.to_csv('high_risk_texts.csv', index=False, encoding='utf-8-sig')
        print(f"  保存 {len(high_risk_texts)} 个高风险文本")

    return features_data

# 生成最终报告
features_data = generate_comprehensive_report(features_data, ngram_analyzer)

# 文字版分析报告
def create_text_analysis_report(features_data):

    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    # 1. 数据概况
    print("数据概况")
    print(f"总样本数: {len(features_data)}")
    print(f"特征数量: {len(score_columns)}")
    print(f"可用特征: {', '.join(score_columns)}")

    # 平台分布
    if 'platform' in features_data.columns:
        platform_counts = features_data['platform'].value_counts()
        print(f"平台分布: {dict(platform_counts)}")

    # 2. 特征统计描述
    print("特征统计描述")

    main_features = ['manipulation_score', 'positive_sentiment_score', 'negative_sentiment_score',
                    'overgeneralization_distortion_score', 'mental_filtering_distortion_score',
                    'catastrophizing_distortion_score', 'labeling_distortion_score',
                    'mind_reading_distortion_score', 'personalization_distortion_score']

    available_features = [f for f in main_features if f in features_data.columns]

    for feature in available_features:
        data = features_data[feature]
        print(f"{feature}:")
        print(f"均值: {data.mean():.4f}")
        print(f"标准差: {data.std():.4f}")
        print(f"最小值: {data.min():.4f}")
        print(f"最大值: {data.max():.4f}")
        print(f"中位数: {data.median():.4f}")
        print(f"非零比例: {(data != 0).mean()*100:.1f}%")

    # 3. 特征相关性分析
    # 准备相关性数据
    corr_data = features_data[score_columns].copy()

    # 处理负值特征
    for col in corr_data.columns:
        if corr_data[col].min() < 0:
            corr_data[col] = corr_data[col].abs()

    # 排除标准差为0的列
    valid_columns = [col for col in corr_data.columns if corr_data[col].std() > 0]
    corr_data = corr_data[valid_columns]

    if len(valid_columns) >= 2:
        correlation_matrix = corr_data.corr()

        # 找出强相关性（绝对值>0.5）
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.5:
                    strong_correlations.append(
                        (correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value)
                    )

        if strong_correlations:
            print("强相关性特征对（|r| > 0.5）:")
            for feat1, feat2, corr in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):
                direction = "正相关" if corr > 0 else "负相关"
                print(f"{feat1} ↔ {feat2}: {corr:.3f} ({direction})")
        else:
            print("未发现强相关性特征对（|r|>0.5）")

        # 相关性总结
        avg_corr = correlation_matrix.values[np.triu_indices_from(correlation_matrix, k=1)].mean()
        print(f"平均相关性强度: {abs(avg_corr):.3f}")
    else:
        print("特征数量不足，无法进行相关性分析")

    # 4. 平台间特征比较
    if 'platform' in features_data.columns and len(features_data['platform'].unique()) > 1:
        platform_features = features_data.groupby('platform')[score_columns].mean()

        print("各平台特征均值:")
        for platform in platform_features.index:
            print(f"{platform}:")
            platform_data = platform_features.loc[platform]
            # 显示前5个最高分的特征
            top_features = platform_data.nlargest(5)
            for feature, value in top_features.items():
                print(f"     {feature}: {value:.4f}")
    else:
        print("平台数据不足或只有一个平台")

    # 5. 认知扭曲类型分析
    distortion_cols = [col for col in score_columns if 'distortion' in col and col in features_data.columns]

    if distortion_cols:
        # 平均分数排序
        distortion_means = features_data[distortion_cols].mean().sort_values(ascending=False)
        distortion_means = distortion_means[distortion_means > 0]

        print("认知扭曲严重程度排名:")
        for i, (feature, mean_val) in enumerate(distortion_means.items(), 1):
            severity = "高" if mean_val > 0.1 else "中" if mean_val > 0.01 else "低"
            print(f"     {i:2d}. {feature}: {mean_val:.4f} ({severity})")

        # 出现频率
        print("认知扭曲出现频率排名:")
        distortion_freq = [(features_data[col] > 0).mean() * 100 for col in distortion_cols]
        distortion_freq_series = pd.Series(distortion_freq, index=distortion_cols)
        distortion_freq_series = distortion_freq_series[distortion_freq_series > 0].sort_values(ascending=False)

        for i, (feature, freq) in enumerate(distortion_freq_series.items(), 1):
            frequency_level = "常见" if freq > 30 else "一般" if freq > 10 else "少见"
            print(f"     {i:2d}. {feature}: {freq:.1f}% ({frequency_level})")
    else:
        print("未找到认知扭曲特征")

    # 6. 特征检测成功率分析
    detection_rates = {}
    for col in score_columns:
        if col in features_data.columns:
            detection_rate = (features_data[col] != 0).mean() * 100
            detection_rates[col] = detection_rate

    if detection_rates:
        rates_series = pd.Series(detection_rates).sort_values(ascending=False)

        print("特征检测成功率排名:")
        for i, (feature, rate) in enumerate(rates_series.items(), 1):
            effectiveness = "优秀" if rate > 70 else "良好" if rate > 30 else "需改进"
            print(f"{i:2d}. {feature}: {rate:.1f}% ({effectiveness})")

        # 总体检测效果
        avg_detection_rate = rates_series.mean()
        overall_rating = "优秀" if avg_detection_rate > 60 else "良好" if avg_detection_rate > 30 else "需改进"
        print(f"总体检测效果: {avg_detection_rate:.1f}% ({overall_rating})")
    else:
        print("无法计算检测成功率")

    # 7. 关键发现总结
    # 找出最重要的发现
    if available_features:
        # 最普遍的特征
        most_common_feature = max([(col, (features_data[col] != 0).mean()) for col in available_features],
                                 key=lambda x: x[1])
        # 最严重的特征
        most_severe_feature = max([(col, features_data[col].mean()) for col in available_features],
                                 key=lambda x: x[1])

        print(f"最普遍的特征: {most_common_feature[0]} ({most_common_feature[1]*100:.1f}%样本中存在)")
        print(f"最严重的特征: {most_severe_feature[0]} (平均强度: {most_severe_feature[1]:.4f})")

        # 数据质量评估
        total_detection_rate = sum((features_data[col] != 0).mean() for col in available_features) / len(available_features) * 100
        quality = "高质量" if total_detection_rate > 50 else "中等质量" if total_detection_rate > 20 else "低质量"
        print(f"数据质量: {total_detection_rate:.1f}% ({quality})")

# 执行文字版分析报告
create_text_analysis_report(features_data)