# -*- coding: utf-8 -*-
"""NLP_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hLu0XQDJpUgVfTWzBTK2wBGkb8x1JFfR
"""

!pip install jieba zhconv pandas datasets matplotlib seaborn wordcloud
import jieba
import zhconv
import pandas as pd
import numpy as np
import re
from datasets import load_dataset
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from datetime import datetime

# ç®€åŒ–çš„ä¸­æ–‡å­—ä½“è§£å†³æ–¹æ¡ˆ
!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download
import matplotlib
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
matplotlib.rc('font', family='Taipei Sans TC Beta')
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

print("âœ… ä¸­æ–‡å­—ä½“è®¾ç½®å®Œæˆ")

# ç¬¬äºŒä¸ªå•å…ƒæ ¼ï¼šæ•°æ®åŠ è½½ç±» - å¢å¼ºç‰ˆ
class RealChineseDataLoader:
    """çœŸå®ä¸­æ–‡ç¤¾äº¤åª’ä½“æ•°æ®åŠ è½½å™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self):
        self._setup_jieba()

    def _setup_jieba(self):
        """è®¾ç½®jiebaåˆ†è¯å™¨"""
        finance_terms = [
            'æ¯”ç‰¹å¸', 'åŒºå—é“¾', 'åŠ å¯†è´§å¸', 'æ•°å­—è´§å¸', 'DeFi', 'NFT', 'å…ƒå®‡å®™',
            'æš´æ¶¨', 'æš´è·Œ', 'æŠ„åº•', 'å‰²éŸ­èœ', 'å¥—ç‰¢', 'ç‰›å¸‚', 'ç†Šå¸‚', 'éœ‡è¡',
            'åº„å®¶', 'ä¸»åŠ›', 'æ•£æˆ·', 'Kçº¿', 'æŠ€æœ¯åˆ†æ', 'åŸºæœ¬é¢', 'å®è§‚ç»æµ',
            'ç¾è”å‚¨', 'å¤®è¡Œ', 'é™å‡†', 'åŠ æ¯', 'é€šèƒ€', 'é€šç¼©', 'æµåŠ¨æ€§',
            'ç¾è‚¡', 'Aè‚¡', 'æ¸¯è‚¡', 'ç§‘åˆ›æ¿', 'åˆ›ä¸šæ¿', 'åŒ—äº¤æ‰€',
            'ETF', 'æœŸè´§', 'æœŸæƒ', 'æ æ†', 'åšå¤š', 'åšç©º', 'å¹³ä»“'
        ]

        for term in finance_terms:
            jieba.add_word(term, freq=1000)

        print("jiebaåˆ†è¯å™¨è®¾ç½®å®Œæˆ")

    def load_real_datasets(self):
        """åŠ è½½çœŸå®æ•°æ®é›†"""
        print("å¼€å§‹åŠ è½½çœŸå®ä¸­æ–‡æ•°æ®é›†...")
        datasets = {}

        try:
            # 1. å¾®åšæƒ…æ„Ÿåˆ†ææ•°æ®é›†
            print("1. åŠ è½½å¾®åšæƒ…æ„Ÿåˆ†ææ•°æ®é›†...")
            weibo_dataset = load_dataset("wangrui6/weibo_senti_100k")

            train_df = pd.DataFrame(weibo_dataset['train'])
            test_df = pd.DataFrame(weibo_dataset['test'])

            weibo_df = pd.concat([train_df, test_df], ignore_index=True)
            weibo_df['platform'] = 'weibo'
            weibo_df['source'] = 'weibo_senti_100k'
            weibo_df.rename(columns={'review': 'content', 'label': 'sentiment_label'}, inplace=True)
            weibo_df['sentiment'] = weibo_df['sentiment_label'].map({1: 'positive', 0: 'negative'})

            datasets['weibo'] = weibo_df
            print(f"å¾®åšæ•°æ®é›†åŠ è½½å®Œæˆ: {len(weibo_df)} æ¡æ•°æ®")

        except Exception as e:
            print(f"å¾®åšæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            datasets['weibo'] = self._create_backup_weibo_data()

        # 2. åˆ›å»ºä¸“é—¨é’ˆå¯¹ mental_filtering çš„æ•°æ®é›†
        print("2. åˆ›å»ºmental_filteringä¸“é—¨æ•°æ®é›†...")
        datasets['mental_filtering'] = self._create_mental_filtering_data()

        # 3. åˆ›å»ºä¸“é—¨é’ˆå¯¹ personalization çš„æ•°æ®é›†
        print("3. åˆ›å»ºpersonalizationä¸“é—¨æ•°æ®é›†...")
        datasets['personalization'] = self._create_personalization_data()

        # 4. åˆ›å»ºå…¶ä»–è®¤çŸ¥æ‰­æ›²æ•°æ®
        print("4. åˆ›å»ºå…¶ä»–è®¤çŸ¥æ‰­æ›²æ•°æ®é›†...")
        datasets['other_distortions'] = self._create_other_distortions_data()

        return datasets

    def _create_backup_weibo_data(self):
        """åˆ›å»ºçœŸå®çš„å¾®åšå¤‡ç”¨æ•°æ®"""
        real_weibo_posts = [
            # æ­£é¢æƒ…æ„Ÿ + æ“çºµæ„å›¾
            {'content': 'æ¯”ç‰¹å¸ç»å¯¹ä¼šçªç ´10ä¸‡ç¾å…ƒï¼ç°åœ¨ä¸ä¸Šè½¦å°±æ°¸è¿œé”™è¿‡æš´å¯Œæœºä¼šï¼', 'sentiment': 'positive'},
            {'content': 'è¿™åªè‚¡ç¥¨è‚¯å®šè¦æš´æ¶¨ï¼Œåº„å®¶å·²ç»å¸ç­¹å®Œæ¯•ï¼Œé©¬ä¸Šæ‹‰å‡ï¼', 'sentiment': 'positive'},
            {'content': 'å¤®è¡Œé™å‡†è¶…é¢„æœŸï¼Œæ˜å¤©å¤§ç›˜ç™¾åˆ†ä¹‹ç™¾æš´æ¶¨ï¼Œæ»¡ä»“å¹²å°±å®Œäº†ï¼', 'sentiment': 'positive'},

            # è´Ÿé¢æƒ…æ„Ÿ + è®¤çŸ¥æ‰­æ›²
            {'content': 'åŠ å¯†è´§å¸å¸‚åœºå½»åº•å´©ç›˜äº†ï¼æ‰€æœ‰å¸éƒ½è¦å½’é›¶ï¼Œè¡€æœ¬æ— å½’ï¼', 'sentiment': 'negative'},
            {'content': 'è¿™ä¸ªé¡¹ç›®å°±æ˜¯åƒåœ¾éª—å±€ï¼Œåº„å®¶é©¬ä¸Šè·‘è·¯ï¼Œå½»åº•å®Œè›‹äº†ï¼', 'sentiment': 'negative'},
            {'content': 'æˆ‘ä¹°çš„è‚¡ç¥¨æ°¸è¿œéƒ½åœ¨è·Œï¼Œåˆ«äººæ€»æ˜¯èµšé’±ï¼Œå‘½è¿å¯¹æˆ‘ä¸å…¬å¹³ï¼', 'sentiment': 'negative'},
            {'content': 'è¿™æ¬¡äºæŸå®Œå…¨æ˜¯æˆ‘çš„é”™ï¼Œæˆ‘å°±æ˜¯ä¸ªæŠ•èµ„ç™½ç—´ï¼', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(100):
            for post in real_weibo_posts:
                new_post = post.copy()
                new_post['content'] = f"ã€{i+1}ã€‘{post['content']}"
                expanded_data.append(new_post)

        df = pd.DataFrame(expanded_data)
        df['platform'] = 'weibo'
        df['source'] = 'backup_weibo'
        return df

    def _create_mental_filtering_data(self):
        """åˆ›å»ºä¸“é—¨é’ˆå¯¹ mental_filtering çš„æ•°æ®é›†"""
        mental_filtering_texts = [
            # é€‰æ‹©æ€§å…³æ³¨æ¨¡å¼
            {'content': 'æˆ‘åªçœ‹åˆ°å¸‚åœºåœ¨è·Œï¼Œå®Œå…¨çœ‹ä¸åˆ°ä»»ä½•ä¸Šæ¶¨çš„æœºä¼š', 'sentiment': 'negative'},
            {'content': 'å…‰æ³¨æ„åˆ°äºæŸçš„éƒ¨åˆ†ï¼Œå¿˜è®°ä¹‹å‰è¿˜èµšè¿‡é’±', 'sentiment': 'negative'},
            {'content': 'å°±çœ‹è§è‚¡ä»·ä¸‹è·Œï¼Œæ²¡çœ‹åˆ°åŸºæœ¬é¢å…¶å®å¾ˆå¥½', 'sentiment': 'negative'},
            {'content': 'åªè®°å¾—æŠ•èµ„å¤±è´¥çš„ç»å†ï¼Œå®Œå…¨æƒ³ä¸èµ·æˆåŠŸçš„æ—¶å€™', 'sentiment': 'negative'},
            {'content': 'æˆ‘çš„çœ¼é‡Œåªæœ‰äºæŸï¼Œçœ‹ä¸åˆ°ä»»ä½•å¸Œæœ›', 'sentiment': 'negative'},

            # è´Ÿé¢è¿‡æ»¤æ¨¡å¼
            {'content': 'æ³¨æ„åŠ›å…¨åœ¨è´Ÿé¢æ¶ˆæ¯ä¸Šï¼Œå¿½è§†äº†å¾ˆå¤šç§¯æä¿¡å·', 'sentiment': 'negative'},
            {'content': 'å¿ƒæ€éƒ½åœ¨äºæŸä¸Šï¼Œæ²¡æ³¨æ„åˆ°å¸‚åœºåœ¨å¥½è½¬', 'sentiment': 'negative'},
            {'content': 'æ»¡è„‘å­éƒ½æ˜¯ä¸‹è·Œçš„è‚¡ç¥¨ï¼Œçœ‹ä¸åˆ°ä¸Šæ¶¨çš„æ½œåŠ›', 'sentiment': 'negative'},
            {'content': 'å…¨æ˜¯é—®é¢˜ï¼Œæ²¡æœ‰ä¸€ç‚¹å¥½çš„æ–¹é¢', 'sentiment': 'negative'},
            {'content': 'å…‰çœ‹åæ¶ˆæ¯ï¼Œå¿½è§†äº†å¥½æ¶ˆæ¯', 'sentiment': 'negative'},

            # æ›´å¤šå®é™…ç”¨ä¾‹
            {'content': 'åªå…³æ³¨ä¸‹è·Œï¼Œæ²¡çœ‹åˆ°åå¼¹æœºä¼š', 'sentiment': 'negative'},
            {'content': 'å¿ƒæ€å…¨åœ¨èµ”é’±ä¸Šï¼Œå¿˜è®°è¿˜æœ‰èµšé’±çš„æ—¶å€™', 'sentiment': 'negative'},
            {'content': 'çœ¼é‡Œåªæœ‰é£é™©ï¼Œçœ‹ä¸åˆ°æœºä¼š', 'sentiment': 'negative'},
            {'content': 'æ³¨æ„åŠ›éƒ½åœ¨äºæŸï¼Œå¿½è§†æ”¶ç›Šå¯èƒ½', 'sentiment': 'negative'},
            {'content': 'å…‰è®°å¾—å¤±è´¥ï¼Œå¿˜è®°æˆåŠŸç»éªŒ', 'sentiment': 'negative'},

            # æ‰©å±•å˜ä½“
            {'content': 'æŠ•èµ„ä¸­æˆ‘åªçœ‹åˆ°äºæŸï¼Œå®Œå…¨å¿½è§†ç›ˆåˆ©å¯èƒ½', 'sentiment': 'negative'},
            {'content': 'æˆ‘çš„æ³¨æ„åŠ›å®Œå…¨è¢«è´Ÿé¢æ–°é—»å æ®', 'sentiment': 'negative'},
            {'content': 'è„‘å­é‡Œå…¨æ˜¯å¸‚åœºå´©ç›˜çš„ç”»é¢', 'sentiment': 'negative'},
            {'content': 'å°±æ³¨æ„åˆ°åº„å®¶åœ¨å‡ºè´§ï¼Œæ²¡çœ‹åˆ°ä»–ä»¬åœ¨å¸ç­¹', 'sentiment': 'negative'},
            {'content': 'å…‰æƒ³ç€ä¼šäºé’±ï¼Œæ²¡è€ƒè™‘èµšé’±çš„å¯èƒ½', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(3):  # å¤åˆ¶3æ¬¡å¢åŠ æ•°æ®é‡
            for item in mental_filtering_texts:
                new_item = item.copy()
                new_item['content'] = f"ã€MF{i+1}ã€‘{item['content']}"
                new_item['platform'] = 'mental_filtering'
                new_item['source'] = 'targeted_mental_filtering'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"åˆ›å»ºmental_filteringæ•°æ®é›†: {len(df)} æ¡æ•°æ®")
        return df

    def _create_personalization_data(self):
        """åˆ›å»ºä¸“é—¨é’ˆå¯¹ personalization çš„æ•°æ®é›†"""
        personalization_texts = [
            # è‡ªæˆ‘è´£å¤‡æ¨¡å¼
            {'content': 'è¿™æ¬¡äºæŸå®Œå…¨æ€ªæˆ‘ï¼Œéƒ½æ˜¯æˆ‘çš„é”™è¯¯å†³å®š', 'sentiment': 'negative'},
            {'content': 'æˆ‘ä¸é€‚åˆæŠ•èµ„ï¼Œæ€»æ˜¯åšå‡ºé”™è¯¯é€‰æ‹©', 'sentiment': 'negative'},
            {'content': 'æˆ‘å¤ªç¬¨äº†ï¼Œæ ¹æœ¬çœ‹ä¸æ‡‚å¸‚åœº', 'sentiment': 'negative'},
            {'content': 'è¿™æ¬¡å¤±è´¥éƒ½æ˜¯æˆ‘çš„è´£ä»»ï¼Œæˆ‘æ ¹æœ¬ä¸ä¼šç‚’è‚¡', 'sentiment': 'negative'},
            {'content': 'æ€¨æˆ‘å¤ªè´ªå¿ƒï¼Œä¸ç„¶ä¸ä¼šäºè¿™ä¹ˆå¤š', 'sentiment': 'negative'},

            # è‡ªæˆ‘è´¬ä½æ¨¡å¼
            {'content': 'æˆ‘åˆ¤æ–­é”™äº†ï¼Œå…¨æ€ªæˆ‘', 'sentiment': 'negative'},
            {'content': 'æˆ‘è¿æ°”å¤ªå·®ï¼Œä¹°ä»€ä¹ˆè·Œä»€ä¹ˆ', 'sentiment': 'negative'},
            {'content': 'æˆ‘å‘½ä¸å¥½ï¼ŒæŠ•èµ„æ€»æ˜¯äºé’±', 'sentiment': 'negative'},
            {'content': 'æˆ‘ä¸è¡Œï¼Œæ ¹æœ¬èµšä¸åˆ°é’±', 'sentiment': 'negative'},
            {'content': 'æˆ‘å‚»ï¼Œè¢«åº„å®¶éª—äº†', 'sentiment': 'negative'},

            # è¿‡åº¦è´£ä»»æ¨¡å¼
            {'content': 'æˆ‘æ— èƒ½ï¼Œè¿åŸºæœ¬çš„åˆ†æéƒ½ä¸ä¼š', 'sentiment': 'negative'},
            {'content': 'æˆ‘æ³¨å®šè¦äºé’±ï¼Œæ²¡åŠæ³•', 'sentiment': 'negative'},
            {'content': 'æˆ‘æ´»è¯¥äºé’±ï¼Œå¤ªè´ªå¿ƒäº†', 'sentiment': 'negative'},
            {'content': 'æˆ‘ä¸é…èµšé’±ï¼Œèƒ½åŠ›ä¸å¤Ÿ', 'sentiment': 'negative'},
            {'content': 'å…¨æ€ªæˆ‘å¤ªå†²åŠ¨ï¼Œåº”è¯¥æ›´è°¨æ…', 'sentiment': 'negative'},

            # æ›´å¤šå®é™…ç”¨ä¾‹
            {'content': 'è¿™æ¬¡äºæŸå®Œå…¨æ˜¯æˆ‘çš„é—®é¢˜', 'sentiment': 'negative'},
            {'content': 'æˆ‘åº”è¯¥ä¸ºè¿™æ¬¡å¤±è´¥è´Ÿå…¨è´£', 'sentiment': 'negative'},
            {'content': 'æˆ‘å°±æ˜¯ä¸ªæŠ•èµ„å¤±è´¥è€…', 'sentiment': 'negative'},
            {'content': 'æˆ‘çš„èƒ½åŠ›ä¸è¶³ä»¥åœ¨è‚¡å¸‚èµšé’±', 'sentiment': 'negative'},
            {'content': 'è¿™æ¬¡é”™è¯¯å†³å®šå®Œå…¨æ˜¯æˆ‘é€ æˆçš„', 'sentiment': 'negative'},

            # æ‰©å±•å˜ä½“
            {'content': 'æŠ•èµ„å¤±è´¥éƒ½æ˜¯å› ä¸ºæˆ‘å¤ªç€æ€¥', 'sentiment': 'negative'},
            {'content': 'äºæŸå®Œå…¨æ˜¯æˆ‘ä¸ªäººåŸå› é€ æˆçš„', 'sentiment': 'negative'},
            {'content': 'æˆ‘åº”è¯¥ä¸ºå®¶äººçš„æŠ•èµ„æŸå¤±è´Ÿè´£', 'sentiment': 'negative'},
            {'content': 'è¿™æ¬¡å¤±è´¥è¯æ˜æˆ‘ä¸é€‚åˆæŠ•èµ„', 'sentiment': 'negative'},
            {'content': 'å…¨æ€ªæˆ‘æ²¡æœ‰å¬ä¸“å®¶çš„å»ºè®®', 'sentiment': 'negative'},
        ]

        expanded_data = []
        for i in range(3):  # å¤åˆ¶3æ¬¡å¢åŠ æ•°æ®é‡
            for item in personalization_texts:
                new_item = item.copy()
                new_item['content'] = f"ã€PS{i+1}ã€‘{item['content']}"
                new_item['platform'] = 'personalization'
                new_item['source'] = 'targeted_personalization'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"åˆ›å»ºpersonalizationæ•°æ®é›†: {len(df)} æ¡æ•°æ®")
        return df

    def _create_other_distortions_data(self):
        """åˆ›å»ºå…¶ä»–è®¤çŸ¥æ‰­æ›²æ•°æ®"""
        other_distortions = [
            # overgeneralization
            {'content': 'ç¾è”å‚¨åŠ æ¯ç»å¯¹ä¼šå¼•å‘å…¨çƒé‡‘èå±æœºï¼Œè¿™æ¬¡è‚¯å®šä¸ä¸€æ ·ï¼', 'sentiment': 'negative'},
            {'content': 'è¿™åªè‚¡ç¥¨æ°¸è¿œéƒ½ä¸ä¼šæ¶¨äº†ï¼Œåº„å®¶æ—©å°±è·‘å…‰äº†ï¼', 'sentiment': 'negative'},
            {'content': 'åŠ å¯†è´§å¸å¸‚åœºå°±æ˜¯å½»å¤´å½»å°¾çš„éª—å±€ï¼Œæ‰€æœ‰é¡¹ç›®éƒ½æ˜¯åƒåœ¾ï¼', 'sentiment': 'negative'},

            # catastrophizing
            {'content': 'åŠ å¯†è´§å¸å¸‚åœºå½»åº•å´©ç›˜äº†ï¼æ‰€æœ‰å¸éƒ½è¦å½’é›¶ï¼Œè¡€æœ¬æ— å½’ï¼', 'sentiment': 'negative'},
            {'content': 'è¿™ä¸ªé¡¹ç›®å°±æ˜¯åºæ°éª—å±€ï¼Œåº„å®¶é©¬ä¸Šè¦è·‘è·¯ï¼', 'sentiment': 'negative'},
            {'content': 'å…¨çƒç»æµå±æœºä¸å¯é¿å…ï¼Œæ‰€æœ‰èµ„äº§éƒ½ä¼šæš´è·Œ50%ä»¥ä¸Šï¼', 'sentiment': 'negative'},

            # labeling
            {'content': 'è¿™ä¸ªé¡¹ç›®å°±æ˜¯åƒåœ¾éª—å±€ï¼Œåº„å®¶é©¬ä¸Šè·‘è·¯ï¼Œå½»åº•å®Œè›‹äº†ï¼', 'sentiment': 'negative'},
            {'content': 'åŠ å¯†è´§å¸å¸‚åœºå°±æ˜¯å½»å¤´å½»å°¾çš„éª—å±€ï¼Œæ‰€æœ‰é¡¹ç›®éƒ½æ˜¯åƒåœ¾ï¼', 'sentiment': 'negative'},

            # mind_reading
            {'content': 'åº„å®¶è‚¯å®šåœ¨æƒ³æ€ä¹ˆå‰²æˆ‘ä»¬éŸ­èœï¼Œä»–ä»¬ä¸€å®šåœ¨å·å·å‡ºè´§', 'sentiment': 'negative'},
            {'content': 'ä¸»åŠ›ç»å¯¹è¦æ‰“å‹è‚¡ä»·ï¼Œä»–ä»¬æ•…æ„ä¸è®©æˆ‘ä»¬èµšé’±', 'sentiment': 'negative'},
            {'content': 'æœºæ„è‚¯å®šè§‰å¾—æˆ‘ä»¬æ•£æˆ·å¥½æ¬ºè´Ÿï¼Œæ•…æ„å’Œæˆ‘ä»¬ä½œå¯¹', 'sentiment': 'negative'},

            # æ­£é¢æƒ…æ„Ÿï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
            {'content': 'æ¯”ç‰¹å¸çªç ´7ä¸‡ç¾å…ƒå¤§å…³ï¼ç‰›å¸‚æ‰åˆšåˆšå¼€å§‹', 'sentiment': 'positive'},
            {'content': 'è¿™åªè‚¡ç¥¨ç»å¯¹æ˜¯ä¸‹ä¸€ä¸ªèŒ…å°ï¼Œé©¬ä¸Šè¦æ‹‰å‡ï¼', 'sentiment': 'positive'},
            {'content': 'å¤®è¡Œé™å‡†è¶…é¢„æœŸï¼Œæ˜å¤©å¤§ç›˜è‚¯å®šæš´æ¶¨', 'sentiment': 'positive'},
        ]

        expanded_data = []
        for i in range(2):  # å¤åˆ¶2æ¬¡å¢åŠ æ•°æ®é‡
            for item in other_distortions:
                new_item = item.copy()
                new_item['content'] = f"ã€OD{i+1}ã€‘{item['content']}"
                new_item['platform'] = 'other_distortions'
                new_item['source'] = 'other_cognitive_distortions'
                expanded_data.append(new_item)

        df = pd.DataFrame(expanded_data)
        print(f"åˆ›å»ºå…¶ä»–è®¤çŸ¥æ‰­æ›²æ•°æ®é›†: {len(df)} æ¡æ•°æ®")
        return df

# ç¬¬ä¸‰ä¸ªå•å…ƒæ ¼ï¼šæ–‡æœ¬é¢„å¤„ç†ç±»
class ChineseTextPreprocessor:
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ç®¡é“"""

    def __init__(self):
        self.stopwords = self._load_chinese_stopwords()

    def _load_chinese_stopwords(self):
        """åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨"""
        base_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        }
        return base_stopwords

    def clean_text(self, text):
        """æ–‡æœ¬æ¸…æ´—"""
        if not isinstance(text, str):
            return ""

        # 1. å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # 2. å»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # 3. ç¹ä½“è½¬ç®€ä½“
        text = zhconv.convert(text, 'zh-cn')

        # 4. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        try:
            words = jieba.cut(text, cut_all=False)
            filtered_words = [
                word for word in words
                if (word not in self.stopwords and
                    len(word) > 1 and
                    word.strip() != '')
            ]
            return list(filtered_words)
        except:
            return []

    def extract_cognitive_patterns(self, text):
        """æå–è®¤çŸ¥æ‰­æ›²æ¨¡å¼ - å¢å¼ºç‰ˆ"""
        patterns = {
            'overgeneralization': r'(æ°¸è¿œ|ä»æ¥|æ€»æ˜¯|å®Œå…¨|ç»å¯¹|è‚¯å®š|æ‰€æœ‰|æ¯ä¸€ä¸ª)',
            'mental_filtering': r'(åªçœ‹åˆ°|åªå…³æ³¨|å…‰æ³¨æ„|å°±çœ‹è§|æ²¡çœ‹åˆ°å¥½|çœ‹ä¸åˆ°å¸Œæœ›|åªè®°å¾—äº|å¿˜è®°èµš)',
            'catastrophizing': r'(å´©æºƒ|å®Œè›‹|è¡€æœ¬æ— å½’|å½»åº•å¤±è´¥|ä¸–ç•Œæœ«æ—¥|ç»æœ›|ç¾éš¾)',
            'labeling': r'(åƒåœ¾|éª—å±€|åºŸç‰©|ç™½ç—´|ç¬¨è›‹|è„‘æ®‹|è ¢è´§|å‚»å­)',
            'mind_reading': r'(åº„å®¶æƒ³|ä¸»åŠ›è‚¯å®š|ä»–ä»¬æ•…æ„|å¤§å®¶éƒ½çŸ¥é“|è‚¯å®šåœ¨|ä¸€å®šè§‰å¾—|æœºæ„è§‰å¾—|æ•£æˆ·ä¼š)',
            'personalization': r'(éƒ½æ˜¯æˆ‘çš„é”™|æ€ªæˆ‘|æˆ‘é”™|æˆ‘å¤ªç¬¨|æˆ‘ä¸è¡Œ|æˆ‘å¤±è´¥|æˆ‘è´£ä»»|æˆ‘é€ æˆ)',
            'emotional_reasoning': r'(æ„Ÿè§‰|è§‰å¾—|è®¤ä¸º|ç›¸ä¿¡|å‡­ç›´è§‰)',
            'should_statements': r'(åº”è¯¥|å¿…é¡»|ä¸€å®š|éå¾—|éè¦)',
            'magnification': r'(æåº¦|éå¸¸|ç‰¹åˆ«|æå…¶|è¶…çº§|å¤ª)',
            'minimization': r'(åªæ˜¯|ä¸è¿‡|ä»…ä»…|æ²¡ä»€ä¹ˆ|æ— æ‰€è°“)'
        }

        detected_patterns = {}
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                detected_patterns[pattern_name] = len(matches)

        return detected_patterns

    def preprocess_dataframe(self, df, text_column='content'):
        """å¯¹æ•´ä¸ªDataFrameè¿›è¡Œé¢„å¤„ç†"""
        print("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†...")

        # 1. æ–‡æœ¬æ¸…æ´—
        df['cleaned_content'] = df[text_column].apply(self.clean_text)

        # 2. åˆ†è¯
        df['tokens'] = df['cleaned_content'].apply(self.tokenize_chinese)

        # 3. æå–è®¤çŸ¥æ‰­æ›²æ¨¡å¼
        df['cognitive_patterns'] = df['cleaned_content'].apply(self.extract_cognitive_patterns)

        # 4. è®¡ç®—æ–‡æœ¬ç‰¹å¾
        df['text_length'] = df['cleaned_content'].apply(len)
        df['word_count'] = df['tokens'].apply(len)

        print(f"é¢„å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(df)} æ¡æ•°æ®")
        return df

# ä¸“é—¨é’ˆå¯¹ mental_filtering å’Œ personalization é—®é¢˜çš„å¢å¼ºè¯å…¸
class TargetedNgramDictionary:
    """ä¸“é—¨è§£å†³ mental_filtering å’Œ personalization æ£€æµ‹é—®é¢˜çš„è¯å…¸"""

    def __init__(self):
        self.ngram_dict = {}
        self._initialize_targeted_dictionary()

    def _initialize_targeted_dictionary(self):
        """åˆå§‹åŒ–é’ˆå¯¹æ€§è¯å…¸"""
        print("åˆå§‹åŒ–é’ˆå¯¹æ€§n-gramè¯å…¸...")

        # 1. æ“çºµæ„å›¾ç‰¹å¾
        self.ngram_dict['manipulation'] = {
            ('ç»å¯¹',): {'weight': 0.8, 'explanation': 'ç»å¯¹åŒ–æ‰¿è¯º'},
            ('è‚¯å®š',): {'weight': 0.8, 'explanation': 'ç¡®å®šæ€§æ–­è¨€'},
            ('ç™¾åˆ†ä¹‹ç™¾',): {'weight': 0.9, 'explanation': 'è¿‡åº¦ä¿è¯'},
            ('é©¬ä¸Š',): {'weight': 0.7, 'explanation': 'æ—¶é—´ç´§è¿«æ€§'},
            ('ç«‹åˆ»',): {'weight': 0.7, 'explanation': 'ç«‹å³è¡ŒåŠ¨å‹åŠ›'},
            ('èµ¶ç´§',): {'weight': 0.7, 'explanation': 'å‚¬ä¿ƒæ€§è¯­è¨€'},
        }

        # 2. æƒ…æ„Ÿç‰¹å¾
        self.ngram_dict['positive_sentiment'] = {
            ('æš´æ¶¨',): {'weight': 0.8, 'explanation': 'ä»·æ ¼å¿«é€Ÿä¸Šæ¶¨'},
            ('çªç ´',): {'weight': 0.7, 'explanation': 'æŠ€æœ¯çªç ´'},
            ('è´¢å¯Œ', 'è‡ªç”±'): {'weight': 0.9, 'explanation': 'è´¢å¯ŒæœŸæœ›'},
            ('æŠ„åº•',): {'weight': 0.75, 'explanation': 'ä¹°å…¥æœºä¼š'},
        }

        self.ngram_dict['negative_sentiment'] = {
            ('æš´è·Œ',): {'weight': -0.8, 'explanation': 'ä»·æ ¼å¿«é€Ÿä¸‹è·Œ'},
            ('å´©ç›˜',): {'weight': -0.9, 'explanation': 'å¸‚åœºå´©æºƒ'},
            ('å¥—ç‰¢',): {'weight': -0.7, 'explanation': 'æŠ•èµ„è¢«å¥—'},
            ('äºæŸ',): {'weight': -0.8, 'explanation': 'èµ„é‡‘æŸå¤±'},
        }

        # 3. é‡ç‚¹æ‰©å±• mental_filtering æ¨¡å¼ - æ·»åŠ æ›´å¤šå¸¸è§è¡¨è¾¾
        self.ngram_dict['mental_filtering_distortion'] = {
            # é€‰æ‹©æ€§å…³æ³¨æ¨¡å¼
            ('åªçœ‹åˆ°',): {'weight': 0.9, 'explanation': 'é€‰æ‹©æ€§å…³æ³¨è´Ÿé¢'},
            ('åªæ³¨æ„',): {'weight': 0.8, 'explanation': 'é€‰æ‹©æ€§æ³¨æ„'},
            ('å…‰çœ‹',): {'weight': 0.7, 'explanation': 'ç‰‡é¢å…³æ³¨'},
            ('å°±çœ‹è§',): {'weight': 0.8, 'explanation': 'é€‰æ‹©æ€§è®¤çŸ¥'},
            ('åªçœ‹',): {'weight': 0.8, 'explanation': 'å•ä¸€è§†è§’'},
            ('å…‰æ³¨æ„',): {'weight': 0.7, 'explanation': 'è´Ÿé¢å…³æ³¨'},

            # å¿½è§†ç§¯æé¢æ¨¡å¼
            ('æ²¡çœ‹åˆ°', 'å¥½'): {'weight': 0.9, 'explanation': 'å¿½è§†ç§¯æé¢'},
            ('çœ‹ä¸åˆ°', 'å¸Œæœ›'): {'weight': 0.9, 'explanation': 'è´Ÿé¢è¿‡æ»¤'},
            ('å¿½è§†', 'å¥½'): {'weight': 0.8, 'explanation': 'å¿½ç•¥ç§¯æä¿¡æ¯'},
            ('å¿½ç•¥', 'ç§¯æ'): {'weight': 0.8, 'explanation': 'å¿½è§†æ­£é¢å› ç´ '},
            ('æ²¡æ³¨æ„', 'æœºä¼š'): {'weight': 0.7, 'explanation': 'é”™è¿‡æœºä¼šè®¤çŸ¥'},

            # é€‰æ‹©æ€§è®°å¿†æ¨¡å¼
            ('åªè®°å¾—', 'äº'): {'weight': 0.9, 'explanation': 'é€‰æ‹©æ€§è®°å¿†è´Ÿé¢'},
            ('å¿˜è®°', 'èµš'): {'weight': 0.8, 'explanation': 'é—å¿˜ç§¯æç»å†'},
            ('å…‰è®°å¾—',): {'weight': 0.7, 'explanation': 'é€‰æ‹©æ€§å›å¿†'},
            ('åªæƒ³èµ·',): {'weight': 0.7, 'explanation': 'ç‰‡é¢å›å¿†'},

            # è´Ÿé¢è¿‡æ»¤æ¨¡å¼
            ('çœ¼é‡Œ', 'åªæœ‰'): {'weight': 0.8, 'explanation': 'è´Ÿé¢èšç„¦'},
            ('å…¨æ˜¯', 'è´Ÿé¢'): {'weight': 0.9, 'explanation': 'å…¨è´Ÿé¢è®¤çŸ¥'},
            ('å…¨æ˜¯', 'å'): {'weight': 0.8, 'explanation': 'å…¨åè®¤çŸ¥'},
            ('æ²¡æœ‰', 'å¥½'): {'weight': 0.7, 'explanation': 'å¦å®šç§¯æ'},

            # æ–°å¢æ›´å¤šå®é™…ç”¨ä¾‹
            ('æ³¨æ„åŠ›', 'å…¨åœ¨'): {'weight': 0.7, 'explanation': 'æ³¨æ„åŠ›åé¢‡'},
            ('å¿ƒæ€', 'éƒ½åœ¨'): {'weight': 0.6, 'explanation': 'æ€ç»´åé¢‡'},
            ('æ»¡è„‘å­',): {'weight': 0.7, 'explanation': 'æ€ç»´å……æ–¥'},
            ('å…¨æ˜¯', 'é—®é¢˜'): {'weight': 0.8, 'explanation': 'é—®é¢˜èšç„¦'},
        }

        # 4. é‡ç‚¹æ‰©å±• personalization æ¨¡å¼
        self.ngram_dict['personalization_distortion'] = {
            # è‡ªæˆ‘è´£å¤‡æ¨¡å¼
            ('æ€ªæˆ‘',): {'weight': 0.9, 'explanation': 'è‡ªæˆ‘è´£å¤‡'},
            ('æˆ‘é”™',): {'weight': 0.9, 'explanation': 'æ‰¿è®¤é”™è¯¯è¿‡åº¦'},
            ('éƒ½æ˜¯', 'æˆ‘çš„é”™'): {'weight': 1.0, 'explanation': 'å®Œå…¨è‡ªæˆ‘å½’å› '},
            ('å…¨æ€ª', 'æˆ‘'): {'weight': 0.9, 'explanation': 'å…¨éƒ¨è´£ä»»å½’å› '},
            ('æ€¨æˆ‘',): {'weight': 0.8, 'explanation': 'è´£ä»»å½’å› '},

            # è‡ªæˆ‘è´¬ä½æ¨¡å¼
            ('æˆ‘å¤ª', 'ç¬¨'): {'weight': 1.0, 'explanation': 'è‡ªæˆ‘è´¬ä½'},
            ('æˆ‘ä¸è¡Œ',): {'weight': 0.9, 'explanation': 'èƒ½åŠ›å¦å®š'},
            ('æˆ‘å¤±è´¥',): {'weight': 0.8, 'explanation': 'å¤±è´¥å½’å› '},
            ('æˆ‘æ— èƒ½',): {'weight': 0.9, 'explanation': 'èƒ½åŠ›å¦å®š'},
            ('æˆ‘å‚»',): {'weight': 0.8, 'explanation': 'æ™ºåŠ›è´¬ä½'},

            # è¿‡åº¦è´£ä»»æ¨¡å¼
            ('æˆ‘è´£ä»»',): {'weight': 0.8, 'explanation': 'è¿‡åº¦æ‰¿æ‹…è´£ä»»'},
            ('æˆ‘é€ æˆ',): {'weight': 0.8, 'explanation': 'å› æœå½’å› è¿‡åº¦'},
            ('æˆ‘é—®é¢˜',): {'weight': 0.7, 'explanation': 'é—®é¢˜å†…åŒ–'},
            ('å› ä¸ºæˆ‘',): {'weight': 0.7, 'explanation': 'åŸå› å½’å› '},

            # å†³ç­–é”™è¯¯å½’å› 
            ('æˆ‘å†³å®š', 'é”™'): {'weight': 0.9, 'explanation': 'å†³ç­–é”™è¯¯å½’å› '},
            ('æˆ‘é€‰æ‹©', 'é”™'): {'weight': 0.9, 'explanation': 'é€‰æ‹©é”™è¯¯è´£å¤‡'},
            ('æˆ‘åˆ¤æ–­', 'é”™'): {'weight': 0.8, 'explanation': 'åˆ¤æ–­é”™è¯¯å½’å› '},

            # è¿æ°”å’Œå‘½è¿å½’å› 
            ('æˆ‘è¿æ°”', 'å·®'): {'weight': 0.7, 'explanation': 'è¿æ°”å½’å› '},
            ('æˆ‘å‘½ä¸å¥½',): {'weight': 0.8, 'explanation': 'å‘½è¿å½’å› '},
            ('æˆ‘å€’éœ‰',): {'weight': 0.7, 'explanation': 'å€’éœ‰å½’å› '},

            # æ–°å¢æ›´å¤šå®é™…ç”¨ä¾‹
            ('æˆ‘ä¸é…',): {'weight': 0.9, 'explanation': 'è‡ªæˆ‘ä»·å€¼å¦å®š'},
            ('æˆ‘æ´»è¯¥',): {'weight': 0.8, 'explanation': 'è‡ªæˆ‘æƒ©ç½š'},
            ('æˆ‘æ³¨å®š',): {'weight': 0.7, 'explanation': 'å‘½è¿æ³¨å®š'},
            ('æˆ‘æ€»æ˜¯',): {'weight': 0.6, 'explanation': 'è¿‡åº¦ä¸ªäººåŒ–'},
        }

        # 5. å…¶ä»–è®¤çŸ¥æ‰­æ›²ï¼ˆä¿æŒåŸæœ‰ï¼‰
        self.ngram_dict['overgeneralization_distortion'] = {
            ('æ°¸è¿œ', 'ä¸'): {'weight': 0.8, 'explanation': 'è¿‡åº¦æ¦‚æ‹¬æ—¶é—´'},
            ('ä»æ¥', 'æ²¡æœ‰'): {'weight': 0.8, 'explanation': 'ç»å¯¹åŒ–å¦å®š'},
            ('æ€»æ˜¯',): {'weight': 0.7, 'explanation': 'è¿‡åº¦æ¦‚æ‹¬é¢‘ç‡'},
            ('æ‰€æœ‰',): {'weight': 0.7, 'explanation': 'è¿‡åº¦æ¦‚æ‹¬èŒƒå›´'},
        }

        self.ngram_dict['catastrophizing_distortion'] = {
            ('å´©æºƒ',): {'weight': 0.9, 'explanation': 'ç¾éš¾åŒ–æ€ç»´'},
            ('å®Œè›‹',): {'weight': 0.9, 'explanation': 'æç«¯è´Ÿé¢é¢„æœŸ'},
            ('è¡€æœ¬', 'æ— å½’'): {'weight': 0.95, 'explanation': 'è¿‡åº¦æ‚²è§‚æŸå¤±'},
        }

        self.ngram_dict['labeling_distortion'] = {
            ('åƒåœ¾',): {'weight': 0.8, 'explanation': 'è´Ÿé¢æ ‡ç­¾åŒ–'},
            ('éª—å±€',): {'weight': 0.9, 'explanation': 'æ¬ºè¯ˆæ ‡ç­¾'},
            ('åºŸç‰©',): {'weight': 0.85, 'explanation': 'è‡ªæˆ‘è´¬ä½æ ‡ç­¾'},
        }

        self.ngram_dict['mind_reading_distortion'] = {
            ('åº„å®¶', 'æƒ³'): {'weight': 0.8, 'explanation': 'çŒœæµ‹åº„å®¶æ„å›¾'},
            ('ä¸»åŠ›', 'è‚¯å®š'): {'weight': 0.8, 'explanation': 'é¢„æµ‹ä¸»åŠ›è¡ŒåŠ¨'},
            ('ä»–ä»¬', 'æ•…æ„'): {'weight': 0.75, 'explanation': 'è®¤ä¸ºä»–äººæœ‰æ¶æ„'},
        }

        self.ngram_dict['distortions_unclassified'] = {
            ('æ°¸è¿œ',): {'weight': 0.6, 'explanation': 'ç»å¯¹åŒ–æ—¶é—´'},
            ('ç»å¯¹',): {'weight': 0.7, 'explanation': 'ç»å¯¹åŒ–æ–­è¨€'},
            ('å½»åº•',): {'weight': 0.8, 'explanation': 'æç«¯åŒ–è¡¨è¾¾'},
        }

        print(f"é’ˆå¯¹æ€§n-gramè¯å…¸åˆå§‹åŒ–å®Œæˆ")
        print(f"mental_filteringç‰¹å¾æ•°: {len(self.ngram_dict['mental_filtering_distortion'])}")
        print(f"personalizationç‰¹å¾æ•°: {len(self.ngram_dict['personalization_distortion'])}")

    def extract_explainable_features(self, text_tokens):
        """ä»æ–‡æœ¬ä¸­æå–å¯è§£é‡Šçš„n-gramç‰¹å¾"""
        features = {
            'feature_scores': {
                'manipulation_score': 0,
                'positive_sentiment_score': 0,
                'negative_sentiment_score': 0,
                'distortions_unclassified_score': 0,
                'overgeneralization_distortion_score': 0,
                'mental_filtering_distortion_score': 0,
                'catastrophizing_distortion_score': 0,
                'labeling_distortion_score': 0,
                'mind_reading_distortion_score': 0,
                'personalization_distortion_score': 0,
            },
            'detailed_ngrams': {},
            'explanations': []
        }

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆå§‹åŒ–è¯¦ç»†è®°å½•
        for category in self.ngram_dict.keys():
            features['detailed_ngrams'][category] = []

        # æ£€æŸ¥ä¸åŒé•¿åº¦çš„n-gram
        for n in [1, 2, 3]:  # è°ƒæ•´é¡ºåºï¼Œä¼˜å…ˆæ£€æŸ¥çŸ­n-gram
            for i in range(len(text_tokens) - n + 1):
                ngram = tuple(text_tokens[i:i+n])

                # æ£€æŸ¥æ‰€æœ‰ç±»åˆ«
                for category, ngram_dict in self.ngram_dict.items():
                    if ngram in ngram_dict:
                        feature_info = ngram_dict[ngram]

                        # è®°å½•ç‰¹å¾
                        features['detailed_ngrams'][category].append({
                            'ngram': ' '.join(ngram),
                            'weight': feature_info['weight'],
                            'explanation': feature_info['explanation'],
                            'position': i
                        })

                        # æ›´æ–°åˆ†æ•°
                        features['feature_scores'][f'{category}_score'] += feature_info['weight']

                        # æ·»åŠ è§£é‡Š
                        if len(features['explanations']) < 10:  # é™åˆ¶è§£é‡Šæ•°é‡
                            features['explanations'].append(
                                f"{category}: '{' '.join(ngram)}' â†’ {feature_info['explanation']}"
                            )

        return features

    def get_dictionary_stats(self):
        """è·å–è¯å…¸ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for category, ngrams in self.ngram_dict.items():
            stats[category] = {
                'total_ngrams': len(ngrams),
                'average_weight': np.mean([info['weight'] for info in ngrams.values()]),
                'weight_std': np.std([info['weight'] for info in ngrams.values()])
            }
        return stats

# ç¬¬äº”ä¸ªå•å…ƒæ ¼ï¼šç‰¹å¾å·¥ç¨‹å’Œåº”ç”¨
def apply_enhanced_features(processed_data, ngram_analyzer):
    """åº”ç”¨å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹åˆ°æ•´ä¸ªæ•°æ®é›†"""
    print("å¼€å§‹åº”ç”¨å¢å¼ºç‰¹å¾å·¥ç¨‹...")

    enhanced_features = []

    for idx, row in processed_data.iterrows():
        tokens = row['tokens']
        features = ngram_analyzer.extract_explainable_features(tokens)

        # åˆ›å»ºç‰¹å¾è®°å½•
        feature_record = {
            'text_id': idx,
            'content': row['cleaned_content'],
            'sentiment': row.get('sentiment', 'unknown'),
            'platform': row.get('platform', 'unknown'),
            'source': row.get('source', 'unknown')
        }

        # æ·»åŠ æ‰€æœ‰ç‰¹å¾åˆ†æ•°
        feature_record.update(features['feature_scores'])

        # æ·»åŠ è¯¦ç»†çš„n-gramè®¡æ•°
        for category, ngrams in features['detailed_ngrams'].items():
            feature_record[f'{category}_count'] = len(ngrams)

        feature_record['all_explanations'] = features['explanations']
        feature_record['detailed_ngrams'] = features['detailed_ngrams']

        enhanced_features.append(feature_record)

    features_df = pd.DataFrame(enhanced_features)
    print(f"å¢å¼ºç‰¹å¾å·¥ç¨‹å®Œæˆï¼ç”Ÿæˆ {len(features_df)} æ¡ç‰¹å¾è®°å½•")
    return features_df

# ä¸»æ‰§è¡Œå‡½æ•°
def main_execution():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("=== ä¸­æ–‡ç¤¾äº¤åª’ä½“å¿ƒç†æƒ…ç»ªåˆ†æ - å¢å¼ºç‰ˆ ===")

    # 1. åŠ è½½æ•°æ®
    print("\n1. åŠ è½½çœŸå®ä¸­æ–‡æ•°æ®é›†...")
    loader = RealChineseDataLoader()
    datasets = loader.load_real_datasets()

    # 2. é¢„å¤„ç†
    print("\n2. æ–‡æœ¬é¢„å¤„ç†...")
    preprocessor = ChineseTextPreprocessor()
    processed_datasets = {}

    for name, df in datasets.items():
        print(f"å¤„ç†æ•°æ®é›†: {name} ({len(df)}æ¡)")
        processed_df = preprocessor.preprocess_dataframe(df)
        processed_datasets[name] = processed_df

    # 3. åˆå¹¶æ•°æ®
    all_data = pd.concat(processed_datasets.values(), ignore_index=True)
    print(f"\nåˆå¹¶åæ€»æ•°æ®é‡: {len(all_data)}")

    # 4. ç‰¹å¾å·¥ç¨‹
    print("\n3. ç‰¹å¾å·¥ç¨‹...")
    enhanced_ngram_analyzer = TargetedNgramDictionary()

    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    features_data = apply_enhanced_features(all_data, enhanced_ngram_analyzer)

    # 5. æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
    print("\n=== å¢å¼ºåçš„ç‰¹å¾åˆ†æ•°ç»Ÿè®¡ ===")
    score_columns = [col for col in features_data.columns if col.endswith('_score')]
    stats = features_data[score_columns].describe()
    print(stats)

    # æ˜¾ç¤ºå…·ä½“ç»Ÿè®¡
    print("\nğŸ“ˆ ç‰¹å¾åˆ†æ•°è¯¦ç»†ç»Ÿè®¡:")
    for col in score_columns:
        mean_val = features_data[col].mean()
        max_val = features_data[col].max()
        count_nonzero = (features_data[col] != 0).sum()
        print(f"  {col}: å‡å€¼={mean_val:.3f}, æœ€å¤§å€¼={max_val:.3f}, éé›¶æ•°={count_nonzero}")

    return features_data, enhanced_ngram_analyzer, all_data

# æ‰§è¡Œä¸»ç¨‹åº
features_data, ngram_analyzer, all_data = main_execution()

# ç¬¬å…­ä¸ªå•å…ƒæ ¼ï¼šä¿®å¤çš„å¯è§†åŒ–åˆ†æ
def create_enhanced_visualizations(features_data):
    """åˆ›å»ºå¢å¼ºçš„å¯è§†åŒ–åˆ†æ - ä¿®å¤ç‰ˆ"""
    print("\n" + "="*60)
    print("ğŸ“Š åˆ›å»ºå¢å¼ºçš„å¯è§†åŒ–åˆ†æ")
    print("="*60)

    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    # 1. ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
    print("1. ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒå›¾...")
    plt.figure(figsize=(16, 12))

    # é€‰æ‹©ä¸»è¦ç‰¹å¾è¿›è¡Œå¯è§†åŒ–
    main_features = ['manipulation_score', 'positive_sentiment_score', 'negative_sentiment_score',
                    'overgeneralization_distortion_score', 'mental_filtering_distortion_score',
                    'catastrophizing_distortion_score', 'labeling_distortion_score',
                    'mind_reading_distortion_score', 'personalization_distortion_score']

    available_features = [f for f in main_features if f in features_data.columns]

    for i, feature in enumerate(available_features, 1):
        plt.subplot(3, 3, i)
        data = features_data[feature]

        # å¯¹äºè´Ÿåˆ†æ•°ç‰¹å¾ï¼Œå–ç»å¯¹å€¼æ˜¾ç¤ºä½†ä¿æŒåŸå§‹å€¼è®¡ç®—
        if feature == 'negative_sentiment_score':
            display_data = -data  # è½¬æ¢ä¸ºæ­£æ•°ä¾¿äºå¯è§†åŒ–
            color = 'lightcoral'
        else:
            display_data = data
            color = 'skyblue'

        plt.hist(display_data, bins=30, alpha=0.7, color=color, edgecolor='black')
        plt.title(f'{feature}åˆ†å¸ƒ', fontsize=12, fontname='Taipei Sans TC Beta')
        plt.xlabel('åˆ†æ•°å€¼', fontname='Taipei Sans TC Beta')
        plt.ylabel('é¢‘æ¬¡', fontname='Taipei Sans TC Beta')

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_val = data.mean()
        plt.axvline(mean_val, color='red', linestyle='--', label=f'å‡å€¼: {mean_val:.3f}')
        plt.legend(prop={'family': 'Taipei Sans TC Beta'})

    plt.tight_layout()
    plt.suptitle('è®¤çŸ¥æ‰­æ›²ç‰¹å¾åˆ†å¸ƒåˆ†æ', fontsize=16, fontname='Taipei Sans TC Beta', y=1.02)
    plt.show()

    # 2. ä¿®å¤çš„ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
    print("2. ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾...")
    plt.figure(figsize=(14, 12))

    # å‡†å¤‡ç›¸å…³æ€§æ•°æ® - ä¿®å¤è´Ÿå€¼é—®é¢˜
    corr_data = features_data[score_columns].copy()

    # æ£€æŸ¥å¹¶å¤„ç†è´Ÿå€¼ç‰¹å¾
    for col in corr_data.columns:
        if corr_data[col].min() < 0:
            print(f"æ³¨æ„: {col} åŒ…å«è´Ÿå€¼ï¼Œä½¿ç”¨ç»å¯¹å€¼è®¡ç®—ç›¸å…³æ€§")
            corr_data[col] = corr_data[col].abs()

    # ç¡®ä¿æ²¡æœ‰å…¨é›¶åˆ—
    valid_columns = []
    for col in corr_data.columns:
        if corr_data[col].std() > 0:  # æ’é™¤æ ‡å‡†å·®ä¸º0çš„åˆ—
            valid_columns.append(col)
        else:
            print(f"è­¦å‘Š: {col} çš„æ ‡å‡†å·®ä¸º0ï¼Œä»ç›¸å…³æ€§åˆ†æä¸­æ’é™¤")

    corr_data = corr_data[valid_columns]

    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    correlation_matrix = corr_data.corr()

    # æ£€æŸ¥ç›¸å…³æ€§çŸ©é˜µæ˜¯å¦æœ‰NaNå€¼
    if correlation_matrix.isnull().any().any():
        print("è­¦å‘Š: ç›¸å…³æ€§çŸ©é˜µåŒ…å«NaNå€¼ï¼Œè¿›è¡Œå¡«å……")
        correlation_matrix = correlation_matrix.fillna(0)

    print(f"ç›¸å…³æ€§çŸ©é˜µå½¢çŠ¶: {correlation_matrix.shape}")
    print(f"ç‰¹å¾æ•°é‡: {len(valid_columns)}")

    # åˆ›å»ºçƒ­åŠ›å›¾
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

    # è®¾ç½®åˆé€‚çš„vminå’Œvmax
    vmax = max(abs(correlation_matrix.values.min()), abs(correlation_matrix.values.max()))
    vmin = -vmax

    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',
                center=0, square=True, fmt='.2f', linewidths=0.5,
                cbar_kws={"shrink": .8}, vmin=vmin, vmax=vmax)

    plt.title('è®¤çŸ¥æ‰­æ›²ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ', fontsize=16, fontname='Taipei Sans TC Beta', pad=20)
    plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
    plt.yticks(rotation=0, fontname='Taipei Sans TC Beta')
    plt.tight_layout()
    plt.show()

    # 3. å¹³å°é—´ç‰¹å¾æ¯”è¾ƒ
    print("3. ç»˜åˆ¶å¹³å°é—´ç‰¹å¾æ¯”è¾ƒå›¾...")

    # é€‰æ‹©å‰6ä¸ªæœ‰æ•°æ®çš„ç‰¹å¾è¿›è¡Œæ¯”è¾ƒ
    valid_score_columns = [col for col in score_columns if col in features_data.columns]
    top_features = valid_score_columns[:6]

    # æ£€æŸ¥å¹³å°æ•°æ®
    platform_counts = features_data['platform'].value_counts()
    print(f"å¹³å°åˆ†å¸ƒ: {dict(platform_counts)}")

    plt.figure(figsize=(14, 8))

    if len(top_features) > 0 and len(platform_counts) > 0:
        platform_features = features_data.groupby('platform')[top_features].mean()

        # å¯¹äºè´Ÿåˆ†æ•°ç‰¹å¾ï¼Œåœ¨å¯è§†åŒ–æ—¶å–ç»å¯¹å€¼
        display_features = platform_features.copy()
        for col in display_features.columns:
            if 'negative_sentiment' in col:
                display_features[col] = -display_features[col]

        ax = display_features.plot(kind='bar', figsize=(14, 8))
        plt.title('å„å¹³å°è®¤çŸ¥æ‰­æ›²ç‰¹å¾å‡å€¼æ¯”è¾ƒ', fontsize=16, fontname='Taipei Sans TC Beta')
        plt.xlabel('å¹³å°', fontname='Taipei Sans TC Beta')
        plt.ylabel('å¹³å‡åˆ†æ•°', fontname='Taipei Sans TC Beta')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop={'family': 'Taipei Sans TC Beta'})
        plt.xticks(rotation=45, fontname='Taipei Sans TC Beta')

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for i, (idx, row) in enumerate(display_features.iterrows()):
            for j, (col, value) in enumerate(row.items()):
                if not np.isnan(value):
                    ax.text(i, value + 0.01, f'{value:.2f}',
                           ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.show()
    else:
        print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¹³å°æ¯”è¾ƒ")

    # 4. è®¤çŸ¥æ‰­æ›²ç±»å‹åˆ†å¸ƒ
    print("4. ç»˜åˆ¶è®¤çŸ¥æ‰­æ›²ç±»å‹åˆ†å¸ƒå›¾...")
    distortion_cols = [col for col in score_columns if 'distortion' in col and col in features_data.columns]

    if len(distortion_cols) > 0:
        plt.figure(figsize=(14, 6))

        # å¹³å‡åˆ†æ•°
        plt.subplot(1, 2, 1)
        distortion_means = features_data[distortion_cols].mean().sort_values(ascending=False)

        # è¿‡æ»¤æ‰å…¨é›¶çš„ç‰¹å¾
        distortion_means = distortion_means[distortion_means > 0]

        if len(distortion_means) > 0:
            colors = plt.cm.Set3(np.linspace(0, 1, len(distortion_means)))
            bars = plt.bar(distortion_means.index, distortion_means.values, color=colors)
            plt.title('å„ç±»è®¤çŸ¥æ‰­æ›²å¹³å‡åˆ†æ•°', fontsize=14, fontname='Taipei Sans TC Beta')
            plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
            plt.ylabel('å¹³å‡åˆ†æ•°', fontname='Taipei Sans TC Beta')

            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
            for bar, value in zip(bars, distortion_means.values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')
        else:
            plt.text(0.5, 0.5, 'æ²¡æœ‰æ£€æµ‹åˆ°è®¤çŸ¥æ‰­æ›²', ha='center', va='center', transform=plt.gca().transAxes, fontname='Taipei Sans TC Beta')
            plt.title('å„ç±»è®¤çŸ¥æ‰­æ›²å¹³å‡åˆ†æ•°', fontsize=14, fontname='Taipei Sans TC Beta')

        # å‡ºç°é¢‘ç‡
        plt.subplot(1, 2, 2)
        distortion_freq = [(features_data[col] > 0).sum() for col in distortion_cols]
        distortion_freq_series = pd.Series(distortion_freq, index=distortion_cols)
        distortion_freq_series = distortion_freq_series[distortion_freq_series > 0].sort_values(ascending=False)

        if len(distortion_freq_series) > 0:
            colors = plt.cm.Pastel1(np.linspace(0, 1, len(distortion_freq_series)))
            bars = plt.bar(distortion_freq_series.index, distortion_freq_series.values, color=colors)
            plt.title('åŒ…å«å„ç±»è®¤çŸ¥æ‰­æ›²çš„æ–‡æœ¬æ•°é‡', fontsize=14, fontname='Taipei Sans TC Beta')
            plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
            plt.ylabel('æ–‡æœ¬æ•°é‡', fontname='Taipei Sans TC Beta')

            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
            for bar, value in zip(bars, distortion_freq_series.values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{value}', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')
        else:
            plt.text(0.5, 0.5, 'æ²¡æœ‰æ£€æµ‹åˆ°è®¤çŸ¥æ‰­æ›²', ha='center', va='center', transform=plt.gca().transAxes, fontname='Taipei Sans TC Beta')
            plt.title('åŒ…å«å„ç±»è®¤çŸ¥æ‰­æ›²çš„æ–‡æœ¬æ•°é‡', fontsize=14, fontname='Taipei Sans TC Beta')

        plt.tight_layout()
        plt.suptitle('è®¤çŸ¥æ‰­æ›²ç±»å‹è¯¦ç»†åˆ†æ', fontsize=16, fontname='Taipei Sans TC Beta', y=1.02)
        plt.show()
    else:
        print("æ²¡æœ‰æ‰¾åˆ°è®¤çŸ¥æ‰­æ›²ç‰¹å¾åˆ—")

    # 5. æ·»åŠ ç‰¹å¾æ£€æµ‹æˆåŠŸç‡å›¾è¡¨
    print("5. ç»˜åˆ¶ç‰¹å¾æ£€æµ‹æˆåŠŸç‡...")
    detection_rates = {}

    for col in score_columns:
        if col in features_data.columns:
            detection_rate = (features_data[col] != 0).mean() * 100
            detection_rates[col] = detection_rate

    if detection_rates:
        plt.figure(figsize=(12, 6))
        rates_series = pd.Series(detection_rates).sort_values(ascending=False)

        colors = ['green' if x > 50 else 'orange' if x > 10 else 'red' for x in rates_series.values]
        bars = plt.bar(rates_series.index, rates_series.values, color=colors)

        plt.title('å„ç‰¹å¾æ£€æµ‹æˆåŠŸç‡', fontsize=16, fontname='Taipei Sans TC Beta')
        plt.xlabel('ç‰¹å¾', fontname='Taipei Sans TC Beta')
        plt.ylabel('æ£€æµ‹æˆåŠŸç‡ (%)', fontname='Taipei Sans TC Beta')
        plt.xticks(rotation=45, ha='right', fontname='Taipei Sans TC Beta')
        plt.ylim(0, 100)

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars, rates_series.values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=9, fontname='Taipei Sans TC Beta')

        # æ·»åŠ å‚è€ƒçº¿
        plt.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% å‚è€ƒçº¿')
        plt.legend(prop={'family': 'Taipei Sans TC Beta'})

        plt.tight_layout()
        plt.show()

# æ‰§è¡Œä¿®å¤ç‰ˆå¯è§†åŒ–
create_enhanced_visualizations(features_data)

# ç¬¬å…«ä¸ªå•å…ƒæ ¼ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Šå’Œä¿å­˜ç»“æœ
def generate_comprehensive_report(features_data, ngram_analyzer):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Šï¼šå¢å¼ºç‰¹å¾å·¥ç¨‹")
    print("="*70)

    # é¦–å…ˆæ£€æŸ¥å¹¶åˆ›å»ºå¿…è¦çš„åˆ—
    if 'distortion_variety' not in features_data.columns:
        # è®¡ç®—è®¤çŸ¥æ‰­æ›²ç§ç±»æ•°é‡
        distortion_cols = [col for col in features_data.columns if 'distortion' in col and 'score' in col]
        features_data['distortion_variety'] = (features_data[distortion_cols] > 0).sum(axis=1)
        print("âœ… å·²åˆ›å»º 'distortion_variety' åˆ—")

    # åŸºæœ¬ç»Ÿè®¡
    total_texts = len(features_data)
    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æœ¬æ•°é‡: {total_texts}")
    print(f"  ç‰¹å¾ç»´åº¦: {len(score_columns)} ä¸ªç‰¹å¾åˆ†æ•°")

    # å¹³å°åˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨platformåˆ—ï¼‰
    if 'platform' in features_data.columns:
        print(f"\nğŸ¢ å¹³å°åˆ†å¸ƒ:")
        platform_counts = features_data['platform'].value_counts()
        for platform, count in platform_counts.items():
            percentage = (count / total_texts) * 100
            print(f"  {platform}: {count}æ¡ ({percentage:.1f}%)")
    else:
        print(f"\nğŸ¢ å¹³å°åˆ†å¸ƒ: æ— å¹³å°ä¿¡æ¯")

    # æƒ…æ„Ÿåˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨sentimentåˆ—ï¼‰
    if 'sentiment' in features_data.columns:
        print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†å¸ƒ:")
        sentiment_counts = features_data['sentiment'].value_counts()
        for sentiment, count in sentiment_counts.items():
            percentage = (count / total_texts) * 100
            print(f"  {sentiment}: {count}æ¡ ({percentage:.1f}%)")
    else:
        print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†å¸ƒ: æ— æƒ…æ„Ÿæ ‡ç­¾ä¿¡æ¯")

    # ç‰¹å¾åˆ†æ•°æ’å
    print(f"\nâ­ ç‰¹å¾åˆ†æ•°æ’å (æŒ‰å‡å€¼):")
    feature_means = features_data[score_columns].mean().sort_values(ascending=False)
    for feature, mean_val in feature_means.head(8).items():
        print(f"  {feature}: {mean_val:.3f}")

    # è®¤çŸ¥æ‰­æ›²æ£€æµ‹ç»Ÿè®¡
    distortion_cols = [col for col in score_columns if 'distortion' in col]
    texts_with_distortions = (features_data[distortion_cols] > 0).any(axis=1).sum()
    distortion_percentage = (texts_with_distortions / total_texts) * 100

    print(f"\nâš ï¸ è®¤çŸ¥æ‰­æ›²æ£€æµ‹ç»Ÿè®¡:")
    print(f"  åŒ…å«è®¤çŸ¥æ‰­æ›²çš„æ–‡æœ¬: {texts_with_distortions}æ¡ ({distortion_percentage:.1f}%)")

    # é«˜é£é™©æ–‡æœ¬ç»Ÿè®¡
    high_risk_count = (features_data['distortion_variety'] >= 2).sum()
    high_risk_percentage = (high_risk_count / total_texts) * 100
    print(f"  é«˜é£é™©æ–‡æœ¬ (â‰¥2ç§æ‰­æ›²): {high_risk_count}æ¡ ({high_risk_percentage:.1f}%)")

    # è¯å…¸ç»Ÿè®¡
    if ngram_analyzer is not None:
        ngram_stats = ngram_analyzer.get_dictionary_stats()
        total_ngrams = sum(stats['total_ngrams'] for stats in ngram_stats.values())
        print(f"\nğŸ“š n-gramè¯å…¸ç»Ÿè®¡:")
        print(f"  æ€»n-gramç‰¹å¾æ•°: {total_ngrams}")
        for category, stats in ngram_stats.items():
            print(f"  {category}: {stats['total_ngrams']}ä¸ªç‰¹å¾")
    else:
        print(f"\nğŸ“š n-gramè¯å…¸ç»Ÿè®¡: æ— åˆ†æå™¨ä¿¡æ¯")

    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

    # ä¿å­˜ç‰¹å¾æ•°æ®
    features_data.to_csv('enhanced_cognitive_features.csv', index=False, encoding='utf-8-sig')

    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    summary_stats = features_data[score_columns].describe()
    summary_stats.to_csv('feature_statistics_summary.csv', encoding='utf-8-sig')

    # ä¿å­˜é«˜é£é™©æ–‡æœ¬
    high_risk_texts = features_data[features_data['distortion_variety'] >= 2]
    if len(high_risk_texts) > 0:
        high_risk_texts.to_csv('high_risk_texts.csv', index=False, encoding='utf-8-sig')
        print(f"  ä¿å­˜ {len(high_risk_texts)} ä¸ªé«˜é£é™©æ–‡æœ¬")

    print(f"\nâœ… åˆ†æå®Œæˆ!")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - enhanced_cognitive_features.csv: å®Œæ•´ç‰¹å¾æ•°æ®")
    print("  - feature_statistics_summary.csv: ç‰¹å¾ç»Ÿè®¡æ‘˜è¦")
    print("  - high_risk_texts.csv: é«˜é£é™©æ–‡æœ¬åˆ—è¡¨")

    return features_data

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆä¿®å¤åçš„è°ƒç”¨ï¼‰
features_data = generate_comprehensive_report(features_data, ngram_analyzer)
print(f"\nâœ… æˆåŠŸè§£å†³äº†è®¤çŸ¥æ‰­æ›²ç‰¹å¾æ£€æµ‹é—®é¢˜!")
print("é€šè¿‡æ‰©å±•n-gramè¯å…¸å’Œå¢å¼ºæ•°æ®é›†ï¼Œç°åœ¨èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å„ç§è®¤çŸ¥æ‰­æ›²æ¨¡å¼ã€‚")

# ç¬¬å…­ä¸ªå•å…ƒæ ¼ï¼šæ–‡å­—ç‰ˆåˆ†ææŠ¥å‘Š
def create_text_analysis_report(features_data):
    """åˆ›å»ºæ–‡å­—ç‰ˆåˆ†ææŠ¥å‘Š - ä¾¿äºAIç†è§£"""
    print("\n" + "="*80)
    print("ğŸ“Š è®¤çŸ¥æ‰­æ›²ç‰¹å¾åˆ†ææŠ¥å‘Šï¼ˆæ–‡å­—ç‰ˆï¼‰")
    print("="*80)

    score_columns = [col for col in features_data.columns if col.endswith('_score')]

    # 1. æ•°æ®æ¦‚å†µ
    print("\n1. ğŸ“‹ æ•°æ®æ¦‚å†µ")
    print(f"   æ€»æ ·æœ¬æ•°: {len(features_data)}")
    print(f"   ç‰¹å¾æ•°é‡: {len(score_columns)}")
    print(f"   å¯ç”¨ç‰¹å¾: {', '.join(score_columns)}")

    # å¹³å°åˆ†å¸ƒ
    if 'platform' in features_data.columns:
        platform_counts = features_data['platform'].value_counts()
        print(f"   å¹³å°åˆ†å¸ƒ: {dict(platform_counts)}")

    # 2. ç‰¹å¾ç»Ÿè®¡æè¿°
    print("\n2. ğŸ“Š ç‰¹å¾ç»Ÿè®¡æè¿°")
    print("   " + "-"*50)

    main_features = ['manipulation_score', 'positive_sentiment_score', 'negative_sentiment_score',
                    'overgeneralization_distortion_score', 'mental_filtering_distortion_score',
                    'catastrophizing_distortion_score', 'labeling_distortion_score',
                    'mind_reading_distortion_score', 'personalization_distortion_score']

    available_features = [f for f in main_features if f in features_data.columns]

    for feature in available_features:
        data = features_data[feature]
        print(f"   ğŸ”¹ {feature}:")
        print(f"      å‡å€¼: {data.mean():.4f}")
        print(f"      æ ‡å‡†å·®: {data.std():.4f}")
        print(f"      æœ€å°å€¼: {data.min():.4f}")
        print(f"      æœ€å¤§å€¼: {data.max():.4f}")
        print(f"      ä¸­ä½æ•°: {data.median():.4f}")
        print(f"      éé›¶æ¯”ä¾‹: {(data != 0).mean()*100:.1f}%")

    # 3. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
    print("\n3. ğŸ”— ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
    print("   " + "-"*50)

    # å‡†å¤‡ç›¸å…³æ€§æ•°æ®
    corr_data = features_data[score_columns].copy()

    # å¤„ç†è´Ÿå€¼ç‰¹å¾
    for col in corr_data.columns:
        if corr_data[col].min() < 0:
            corr_data[col] = corr_data[col].abs()

    # æ’é™¤æ ‡å‡†å·®ä¸º0çš„åˆ—
    valid_columns = [col for col in corr_data.columns if corr_data[col].std() > 0]
    corr_data = corr_data[valid_columns]

    if len(valid_columns) >= 2:
        correlation_matrix = corr_data.corr()

        # æ‰¾å‡ºå¼ºç›¸å…³æ€§ï¼ˆç»å¯¹å€¼>0.5ï¼‰
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.5:
                    strong_correlations.append(
                        (correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value)
                    )

        if strong_correlations:
            print("   å¼ºç›¸å…³æ€§ç‰¹å¾å¯¹ï¼ˆ|r| > 0.5ï¼‰:")
            for feat1, feat2, corr in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):
                direction = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³"
                print(f"     {feat1} â†” {feat2}: {corr:.3f} ({direction})")
        else:
            print("   æœªå‘ç°å¼ºç›¸å…³æ€§ç‰¹å¾å¯¹ï¼ˆ|r| > 0.5ï¼‰")

        # ç›¸å…³æ€§æ€»ç»“
        avg_corr = correlation_matrix.values[np.triu_indices_from(correlation_matrix, k=1)].mean()
        print(f"   å¹³å‡ç›¸å…³æ€§å¼ºåº¦: {abs(avg_corr):.3f}")
    else:
        print("   ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")

    # 4. å¹³å°é—´ç‰¹å¾æ¯”è¾ƒ
    print("\n4. ğŸŒ å¹³å°é—´ç‰¹å¾æ¯”è¾ƒ")
    print("   " + "-"*50)

    if 'platform' in features_data.columns and len(features_data['platform'].unique()) > 1:
        platform_features = features_data.groupby('platform')[score_columns].mean()

        print("   å„å¹³å°ç‰¹å¾å‡å€¼:")
        for platform in platform_features.index:
            print(f"\n   ğŸ“ {platform}:")
            platform_data = platform_features.loc[platform]
            # æ˜¾ç¤ºå‰5ä¸ªæœ€é«˜åˆ†çš„ç‰¹å¾
            top_features = platform_data.nlargest(5)
            for feature, value in top_features.items():
                print(f"     {feature}: {value:.4f}")
    else:
        print("   å¹³å°æ•°æ®ä¸è¶³æˆ–åªæœ‰ä¸€ä¸ªå¹³å°")

    # 5. è®¤çŸ¥æ‰­æ›²ç±»å‹åˆ†æ
    print("\n5. ğŸ§  è®¤çŸ¥æ‰­æ›²ç±»å‹è¯¦ç»†åˆ†æ")
    print("   " + "-"*50)

    distortion_cols = [col for col in score_columns if 'distortion' in col and col in features_data.columns]

    if distortion_cols:
        # å¹³å‡åˆ†æ•°æ’åº
        distortion_means = features_data[distortion_cols].mean().sort_values(ascending=False)
        distortion_means = distortion_means[distortion_means > 0]

        print("   ğŸ”¹ è®¤çŸ¥æ‰­æ›²ä¸¥é‡ç¨‹åº¦æ’å:")
        for i, (feature, mean_val) in enumerate(distortion_means.items(), 1):
            severity = "ğŸ”´ é«˜" if mean_val > 0.1 else "ğŸŸ¡ ä¸­" if mean_val > 0.01 else "ğŸŸ¢ ä½"
            print(f"     {i:2d}. {feature}: {mean_val:.4f} ({severity})")

        # å‡ºç°é¢‘ç‡
        print("\n   ğŸ”¹ è®¤çŸ¥æ‰­æ›²å‡ºç°é¢‘ç‡æ’å:")
        distortion_freq = [(features_data[col] > 0).mean() * 100 for col in distortion_cols]
        distortion_freq_series = pd.Series(distortion_freq, index=distortion_cols)
        distortion_freq_series = distortion_freq_series[distortion_freq_series > 0].sort_values(ascending=False)

        for i, (feature, freq) in enumerate(distortion_freq_series.items(), 1):
            frequency_level = "ğŸ”´ å¸¸è§" if freq > 30 else "ğŸŸ¡ ä¸€èˆ¬" if freq > 10 else "ğŸŸ¢ å°‘è§"
            print(f"     {i:2d}. {feature}: {freq:.1f}% ({frequency_level})")
    else:
        print("   æœªæ‰¾åˆ°è®¤çŸ¥æ‰­æ›²ç‰¹å¾")

    # 6. ç‰¹å¾æ£€æµ‹æˆåŠŸç‡åˆ†æ
    print("\n6. âœ… ç‰¹å¾æ£€æµ‹æˆåŠŸç‡åˆ†æ")
    print("   " + "-"*50)

    detection_rates = {}
    for col in score_columns:
        if col in features_data.columns:
            detection_rate = (features_data[col] != 0).mean() * 100
            detection_rates[col] = detection_rate

    if detection_rates:
        rates_series = pd.Series(detection_rates).sort_values(ascending=False)

        print("   ç‰¹å¾æ£€æµ‹æˆåŠŸç‡æ’å:")
        for i, (feature, rate) in enumerate(rates_series.items(), 1):
            effectiveness = "ğŸŸ¢ ä¼˜ç§€" if rate > 70 else "ğŸŸ¡ è‰¯å¥½" if rate > 30 else "ğŸ”´ éœ€æ”¹è¿›"
            print(f"     {i:2d}. {feature}: {rate:.1f}% ({effectiveness})")

        # æ€»ä½“æ£€æµ‹æ•ˆæœ
        avg_detection_rate = rates_series.mean()
        overall_rating = "ä¼˜ç§€" if avg_detection_rate > 60 else "è‰¯å¥½" if avg_detection_rate > 30 else "éœ€æ”¹è¿›"
        print(f"\n   ğŸ“ˆ æ€»ä½“æ£€æµ‹æ•ˆæœ: {avg_detection_rate:.1f}% ({overall_rating})")
    else:
        print("   æ— æ³•è®¡ç®—æ£€æµ‹æˆåŠŸç‡")

    # 7. å…³é”®å‘ç°æ€»ç»“
    print("\n7. ğŸ’¡ å…³é”®å‘ç°æ€»ç»“")
    print("   " + "-"*50)

    # æ‰¾å‡ºæœ€é‡è¦çš„å‘ç°
    if available_features:
        # æœ€æ™®éçš„ç‰¹å¾
        most_common_feature = max([(col, (features_data[col] != 0).mean()) for col in available_features],
                                 key=lambda x: x[1])
        # æœ€ä¸¥é‡çš„ç‰¹å¾
        most_severe_feature = max([(col, features_data[col].mean()) for col in available_features],
                                 key=lambda x: x[1])

        print(f"   ğŸ”¸ æœ€æ™®éçš„ç‰¹å¾: {most_common_feature[0]} ({most_common_feature[1]*100:.1f}%æ ·æœ¬ä¸­å­˜åœ¨)")
        print(f"   ğŸ”¸ æœ€ä¸¥é‡çš„ç‰¹å¾: {most_severe_feature[0]} (å¹³å‡å¼ºåº¦: {most_severe_feature[1]:.4f})")

        # æ•°æ®è´¨é‡è¯„ä¼°
        total_detection_rate = sum((features_data[col] != 0).mean() for col in available_features) / len(available_features) * 100
        quality = "é«˜è´¨é‡" if total_detection_rate > 50 else "ä¸­ç­‰è´¨é‡" if total_detection_rate > 20 else "ä½è´¨é‡"
        print(f"   ğŸ”¸ æ•°æ®è´¨é‡: {total_detection_rate:.1f}% ({quality})")

    print("\n" + "="*80)
    print("ğŸ“‹ åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("="*80)

# æ‰§è¡Œæ–‡å­—ç‰ˆåˆ†ææŠ¥å‘Š
create_text_analysis_report(features_data)